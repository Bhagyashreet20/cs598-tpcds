{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "script to combine NLQ used while charactrization study and gpt generated 10 different NLQ for a single query to form a comlee daatset\n",
    "each entry will have an id, golden sql query, schema used for prommpt, NLQ\n",
    "\n",
    "dataset size is 1089. each query has 10 different NLQ. so 99+990\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLQ ->2 folders /workspace/data/tpcds-insights/data/raw/NLQ-Converted/GPT-NLQ, /workspace/data/tpcds-insights/data/raw/NLQ-Converted/GPT-NLQ-Augmented\n",
    "#schema ->/workspace/data/duckdb/results/tpcds/prompt_schema_info\n",
    "#godlen SQL - /workspace/data/tpcds-insights/data/sql/sql_files_sf100\n",
    "#\"query-table-names \"/root/golden_query_tables.txt\"\n",
    "\n",
    "# {\n",
    "#     'id':'q1_1',\n",
    "#     'nlq':'',\n",
    "#     'schema':'',\n",
    "#     'golden':''\n",
    "#     'query_table_names':''\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLQ_path = \"/workspace/data/tpcds-insights/data/raw/NLQ-Converted/GPT-NLQ/\"\n",
    "augmented_path =\"/workspace/data/tpcds-insights/data/raw/NLQ-Converted/GPT-NLQ-Augmented/\"\n",
    "schema_path= \"/workspace/data/duckdb/results/tpcds/prompt_schema_info\"\n",
    "godlen_sql_path = \"/workspace/data/tpcds-insights/data/sql/sql_files_sf100\"\n",
    "\n",
    "\n",
    "#\"query-table-names \"/root/golden_query_tables.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n"
     ]
    }
   ],
   "source": [
    "dataset=[]\n",
    "for queryIdx in range(99):\n",
    "    #opening NLQ file\n",
    "    query={}\n",
    "    query[\"id\"]=f\"q{queryIdx+1}_1\"\n",
    "    nlq_file_path = os.path.join(NLQ_path, f'query{queryIdx+1}.txt')\n",
    "    with open(nlq_file_path, \"r\") as nlq_file:\n",
    "        nlq_file_content = nlq_file.read()\n",
    "        #print(\"opend\",{queryIdx+1})\n",
    "        query[\"nlq\"]=nlq_file_content\n",
    "\n",
    "    #opening golden query\n",
    "    golden_file_path = os.path.join(godlen_sql_path, f'query_{queryIdx+1}.sql')\n",
    "    with open(golden_file_path, \"r\") as golden_sql_file:\n",
    "        golden_file_content = golden_sql_file.read()\n",
    "        #print(\"golden sql opend\",{queryIdx+1})\n",
    "        query[\"sql\"]=golden_file_content\n",
    "\n",
    "    #opening schema for the query\n",
    "    schema_file_path = os.path.join(schema_path, f'query_{queryIdx+1}.txt')\n",
    "    with open(schema_file_path, \"r\") as schema_file:\n",
    "        schema_file_content = schema_file.read()\n",
    "        #print(\"schema_file  opend\",{queryIdx+1})\n",
    "        query[\"schema\"]=schema_file_content\n",
    "\n",
    "    dataset.append(query)\n",
    "    \n",
    "    #opening augmented NLQ\n",
    "  \n",
    "    aug_file_path = os.path.join(augmented_path, f'query{queryIdx+1}.txt') \n",
    "    with open(aug_file_path, \"r\") as aug_file:\n",
    "        aug_file_content = aug_file.readlines()\n",
    "        modified_aug_file_content=[line for line in aug_file_content if line!='\\n']\n",
    "       \n",
    "       \n",
    "        for augIdx in range(len(modified_aug_file_content)):\n",
    "            modified_aug_file_content[augIdx]=modified_aug_file_content[augIdx].replace(f\"Description {augIdx+1}:\",\"\")\n",
    "            \n",
    "        #print(\"aug opend\",{augQueryIdx+1})\n",
    "        \n",
    "            aug_query={}\n",
    "            aug_query[\"nlq\"]=modified_aug_file_content[augIdx]\n",
    "            aug_query[\"id\"]=f\"q{queryIdx+1}_{augIdx+2}\"\n",
    "            aug_query[\"sql\"]=golden_file_content\n",
    "            aug_query[\"schema\"]=schema_file_content\n",
    "        \n",
    "            dataset.append(aug_query)\n",
    "\n",
    "\n",
    "\n",
    "# for i in dataset:\n",
    "#     print(i,\"\\n\")\n",
    "print(len(dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save entire dataset as json\n",
    "import json\n",
    "\n",
    "\n",
    "file_path = \"/workspace/data/finetuning/dataset/sqlize-dataset.json\"\n",
    "# Convert the array to a JSON-formatted string\n",
    "json_data = json.dumps(dataset, indent=4) \n",
    "\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
