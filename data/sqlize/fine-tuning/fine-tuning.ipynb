{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#FINE-TUNING PRE-TRAINED LLAMA ON TPCDSA DATA\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment this line to save the data splits to their respective locations. Running this will replace the existing content in these folders\n",
    "\n",
    "# train_dataset.save_to_disk(train_datset_dir)\n",
    "# val_dataset.save_to_disk(validation_dataset_dir)\n",
    "# test_dataset.save_to_disk(test_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9186d21b224db7b9b147227834f713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb95f562370041b6b1f997bf09fe9e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f54065e3dd4d3480182af3e2507d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 654\n",
      "Validation dataset size: 218\n",
      "Test dataset size: 217\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#DO-NOT EXECUTE THIS AGAIN> THIS WILL REPLACE THE TRAIN_SPLIT SAVED\n",
    "file_path = \"./dataset/sqlize-finetuned-dataset.json\"\n",
    "train_dataset_dir =\"./dataset/dataset-splits/train-split/\"\n",
    "validation_dataset_dir =\"./dataset/dataset-splits/val-split/\"\n",
    "test_dataset_dir =\"./dataset/dataset-splits/test-split/\"\n",
    "# Load your JSON dataset into a Dataset object\n",
    "dataset = load_dataset('json', data_files=file_path)\n",
    "\n",
    "\n",
    "\n",
    "# train-654, val-218, test-217\n",
    "# Split each dataset within the DatasetDict\n",
    "dataset_split = dataset[\"train\"].train_test_split(\n",
    "    test_size=435,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_dataset=dataset_split[\"train\"]\n",
    "\n",
    "val_and_test_splits=dataset_split[\"test\"].train_test_split(\n",
    "    test_size=217,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_dataset = val_and_test_splits[\"train\"]\n",
    "test_dataset = val_and_test_splits[\"test\"]\n",
    "\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "\n",
    "# train_dataset.save_to_disk(train_datset_dir)\n",
    "# val_dataset.save_to_disk(validation_dataset_dir)\n",
    "# test_dataset.save_to_disk(test_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train_splits=load_from_disk('./dataset//dataset-splits/train-split')\n",
    "val_splits=load_from_disk('./dataset/dataset-splits/val-split')\n",
    "test_splits=load_from_disk('./dataset/dataset-splits/test-split')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '\\n[SYSTEM]:\"You are an expert Text-to-SQL generator assistant. Your goal is to provide correct SQL queries to the given text description. Your output only contains the SQL code. No explanation or introductory sentences surrounding the SQL response is needed. You are given schema information. Here is the schema information: \\n<tableName>item</tableName>\\n<columns>i_item_sk,  i_item_id,  i_rec_start_date,  i_rec_end_date,  i_item_desc,  i_current_price,  i_wholesale_cost,  i_brand_id,  i_brand,  i_class_id,  i_class,  i_category_id,  i_category,  i_manufact_id,  i_manufact,  i_size,  i_formulation,  i_color,  i_units,  i_container,  i_manager_id,  i_product_name</columns>\\n<tableName>store_sales</tableName>\\n<columns>ss_sold_date_sk,  ss_sold_time_sk,  ss_item_sk,  ss_customer_sk,  ss_cdemo_sk,  ss_hdemo_sk,  ss_addr_sk,  ss_store_sk,  ss_promo_sk,  ss_ticket_number,  ss_quantity,  ss_wholesale_cost,  ss_list_price,  ss_sales_price,  ss_ext_discount_amt,  ss_ext_sales_price,  ss_ext_wholesale_cost,  ss_ext_list_price,  ss_ext_tax,  ss_coupon_amt,  ss_net_paid,  ss_net_paid_inc_tax,  ss_net_profit</columns>\\n<tableName>date_dim</tableName>\\n<columns>d_date_sk,  d_date_id,  d_date,  d_month_seq,  d_week_seq,  d_quarter_seq,  d_year,  d_dow,  d_moy,  d_dom,  d_qoy,  d_fy_year,  d_fy_quarter_seq,  d_fy_week_seq,  d_day_name,  d_quarter_name,  d_holiday,  d_weekend,  d_following_holiday,  d_first_dom,  d_last_dom,  d_same_day_ly,  d_same_day_lq,  d_current_day,  d_current_week,  d_current_month,  d_current_quarter,  d_current_year</columns>\\n<tableName>store</tableName>\\n<columns>s_store_sk,  s_store_id,  s_rec_start_date,  s_rec_end_date,  s_closed_date_sk,  s_store_name,  s_number_employees,  s_floor_space,  s_hours,  s_manager,  s_market_id,  s_geography_class,  s_market_desc,  s_market_manager,  s_division_id,  s_division_name,  s_company_id,  s_company_name,  s_street_number,  s_street_name,  s_street_type,  s_suite_number,  s_city,  s_county,  s_state,  s_zip,  s_country,  s_gmt_offset,  s_tax_percentage</columns>\\n. Here are the 5 critical rules for the interactions you must abide: <rules> 1. Do not wrap the generated SQL code within SQL code markdown format. Also, do not include the SQL keyword in the beginning of the response. 2. If I don\\'t tell you to find the limited set of results, limit to 100. 3. Only use table and columns from the list provided 4. When performing aliasing, make sure to refer the aliased tables as alias.column_name and not as alias_column_name. 5. For US state names, use abbreviated forms. For example, for South Dakota state, use SD.</rules> \\n\\n\"Here is the user question:\"[/SYSTEM]\\n[HUMAN]: Generate a report that displays the monthly sales figures for items sold in stores during a specific year. The results should be grouped by the store manager. Additionally, calculate the average monthly sales for each manager and month combination.\\n[/HUMAN]\\n',\n",
       " 'text': '\\n\\n[SYSTEM]:\"You are an expert Text-to-SQL generator assistant. Your goal is to provide correct SQL queries to the given text description. Your output only contains the SQL code. No explanation or introductory sentences surrounding the SQL response is needed. You are given schema information. Here is the schema information: \\n<tableName>item</tableName>\\n<columns>i_item_sk,  i_item_id,  i_rec_start_date,  i_rec_end_date,  i_item_desc,  i_current_price,  i_wholesale_cost,  i_brand_id,  i_brand,  i_class_id,  i_class,  i_category_id,  i_category,  i_manufact_id,  i_manufact,  i_size,  i_formulation,  i_color,  i_units,  i_container,  i_manager_id,  i_product_name</columns>\\n<tableName>store_sales</tableName>\\n<columns>ss_sold_date_sk,  ss_sold_time_sk,  ss_item_sk,  ss_customer_sk,  ss_cdemo_sk,  ss_hdemo_sk,  ss_addr_sk,  ss_store_sk,  ss_promo_sk,  ss_ticket_number,  ss_quantity,  ss_wholesale_cost,  ss_list_price,  ss_sales_price,  ss_ext_discount_amt,  ss_ext_sales_price,  ss_ext_wholesale_cost,  ss_ext_list_price,  ss_ext_tax,  ss_coupon_amt,  ss_net_paid,  ss_net_paid_inc_tax,  ss_net_profit</columns>\\n<tableName>date_dim</tableName>\\n<columns>d_date_sk,  d_date_id,  d_date,  d_month_seq,  d_week_seq,  d_quarter_seq,  d_year,  d_dow,  d_moy,  d_dom,  d_qoy,  d_fy_year,  d_fy_quarter_seq,  d_fy_week_seq,  d_day_name,  d_quarter_name,  d_holiday,  d_weekend,  d_following_holiday,  d_first_dom,  d_last_dom,  d_same_day_ly,  d_same_day_lq,  d_current_day,  d_current_week,  d_current_month,  d_current_quarter,  d_current_year</columns>\\n<tableName>store</tableName>\\n<columns>s_store_sk,  s_store_id,  s_rec_start_date,  s_rec_end_date,  s_closed_date_sk,  s_store_name,  s_number_employees,  s_floor_space,  s_hours,  s_manager,  s_market_id,  s_geography_class,  s_market_desc,  s_market_manager,  s_division_id,  s_division_name,  s_company_id,  s_company_name,  s_street_number,  s_street_name,  s_street_type,  s_suite_number,  s_city,  s_county,  s_state,  s_zip,  s_country,  s_gmt_offset,  s_tax_percentage</columns>\\n. Here are the 5 critical rules for the interactions you must abide: <rules> 1. Do not wrap the generated SQL code within SQL code markdown format. Also, do not include the SQL keyword in the beginning of the response. 2. If I don\\'t tell you to find the limited set of results, limit to 100. 3. Only use table and columns from the list provided 4. When performing aliasing, make sure to refer the aliased tables as alias.column_name and not as alias_column_name. 5. For US state names, use abbreviated forms. For example, for South Dakota state, use SD.</rules> \\n\\n\"Here is the user question:\"[/SYSTEM]\\n[HUMAN]: Generate a report that displays the monthly sales figures for items sold in stores during a specific year. The results should be grouped by the store manager. Additionally, calculate the average monthly sales for each manager and month combination.\\n[/HUMAN]\\n\\n[SEP] select  * \\nfrom (select i_manager_id\\n             ,sum(ss_sales_price) sum_sales\\n             ,avg(sum(ss_sales_price)) over (partition by i_manager_id) avg_monthly_sales\\n      from item\\n          ,store_sales\\n          ,date_dim\\n          ,store\\n      where ss_item_sk = i_item_sk\\n        and ss_sold_date_sk = d_date_sk\\n        and ss_store_sk = s_store_sk\\n        and d_month_seq in (1181,1181+1,1181+2,1181+3,1181+4,1181+5,1181+6,1181+7,1181+8,1181+9,1181+10,1181+11)\\n        and ((    i_category in (\\'Books\\',\\'Children\\',\\'Electronics\\')\\n              and i_class in (\\'personal\\',\\'portable\\',\\'reference\\',\\'self-help\\')\\n              and i_brand in (\\'scholaramalgamalg #14\\',\\'scholaramalgamalg #7\\',\\n\\t\\t                  \\'exportiunivamalg #9\\',\\'scholaramalgamalg #9\\'))\\n           or(    i_category in (\\'Women\\',\\'Music\\',\\'Men\\')\\n              and i_class in (\\'accessories\\',\\'classical\\',\\'fragrances\\',\\'pants\\')\\n              and i_brand in (\\'amalgimporto #1\\',\\'edu packscholar #1\\',\\'exportiimporto #1\\',\\n\\t\\t                 \\'importoamalg #1\\')))\\ngroup by i_manager_id, d_moy) tmp1\\nwhere case when avg_monthly_sales > 0 then abs (sum_sales - avg_monthly_sales) / avg_monthly_sales else null end > 0.1\\norder by i_manager_id\\n        ,avg_monthly_sales\\n        ,sum_sales\\nlimit 100;\\n\\n',\n",
       " 'output': \"select  * \\nfrom (select i_manager_id\\n             ,sum(ss_sales_price) sum_sales\\n             ,avg(sum(ss_sales_price)) over (partition by i_manager_id) avg_monthly_sales\\n      from item\\n          ,store_sales\\n          ,date_dim\\n          ,store\\n      where ss_item_sk = i_item_sk\\n        and ss_sold_date_sk = d_date_sk\\n        and ss_store_sk = s_store_sk\\n        and d_month_seq in (1181,1181+1,1181+2,1181+3,1181+4,1181+5,1181+6,1181+7,1181+8,1181+9,1181+10,1181+11)\\n        and ((    i_category in ('Books','Children','Electronics')\\n              and i_class in ('personal','portable','reference','self-help')\\n              and i_brand in ('scholaramalgamalg #14','scholaramalgamalg #7',\\n\\t\\t                  'exportiunivamalg #9','scholaramalgamalg #9'))\\n           or(    i_category in ('Women','Music','Men')\\n              and i_class in ('accessories','classical','fragrances','pants')\\n              and i_brand in ('amalgimporto #1','edu packscholar #1','exportiimporto #1',\\n\\t\\t                 'importoamalg #1')))\\ngroup by i_manager_id, d_moy) tmp1\\nwhere case when avg_monthly_sales > 0 then abs (sum_sales - avg_monthly_sales) / avg_monthly_sales else null end > 0.1\\norder by i_manager_id\\n        ,avg_monthly_sales\\n        ,sum_sales\\nlimit 100;\\n\\n\",\n",
       " 'id': 'q63_7'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf57d06e58254aadb90e36e847e426e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe354e38466c4cfabc34eae2b1d42ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#combine base and pretrained model - since pre-training using LORA is like adding new weights to the base model, that is why you need to merge the adapter and base model\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model_id = \"../models/7B/output\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "new_model_id = \"../models/llama-2-7b-pretrained\"\n",
    "new_model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, new_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True,device=\"cuda\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataCollator for Language Modeling to collate and prepare data\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Set this to True if you are fine-tuning for masked language modeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "\n",
    "\n",
    "# The instruction dataset to use\n",
    "\n",
    "\n",
    "# Fine-tuned model name ( fine-tuned on tpcds data)\n",
    "new_model = \"../models/llama-2-7b-finetuned-text2SQL\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./finetuning-results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 5\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:204: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b345b8a9db284badb55237322f86ef7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_splits,\n",
    "    eval_dataset= val_splits,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    "    data_collator=data_collator, \n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/3270 00:04 < 1:15:45, 0.72 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/data/cs598-tpcds/data/sqlize/fine-tuning/fine-tuning.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e6772795f73616e646572736f6e227d@ssh-remote%2Bbeast.csl.illinois.edu/workspace/data/cs598-tpcds/data/sqlize/fine-tuning/fine-tuning.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e6772795f73616e646572736f6e227d@ssh-remote%2Bbeast.csl.illinois.edu/workspace/data/cs598-tpcds/data/sqlize/fine-tuning/fine-tuning.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e6772795f73616e646572736f6e227d@ssh-remote%2Bbeast.csl.illinois.edu/workspace/data/cs598-tpcds/data/sqlize/fine-tuning/fine-tuning.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Save trained model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f616e6772795f73616e646572736f6e227d@ssh-remote%2Bbeast.csl.illinois.edu/workspace/data/cs598-tpcds/data/sqlize/fine-tuning/fine-tuning.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(new_model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:290\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    288\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trl_activate_neftune(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m--> 290\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    292\u001b[0m \u001b[39m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2733\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2736\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    483\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    484\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[0;32m--> 491\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    492\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    493\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history\n",
    "\n",
    "#writing training loss to a file\n",
    "import json\n",
    "\n",
    "with open('./finetuning-results/training_loss.json',\"w\") as file:\n",
    "    file.write(json.dumps(trainer.state.log_history,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting graph for loss\n",
    "\n",
    "log_path = './finetuning-results/training_loss.json'\n",
    "\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "with open(log_path,\"r\") as json_file:\n",
    "    data= json.load(json_file)\n",
    "\n",
    "print(data)\n",
    "\n",
    "steps  = [entry[\"step\"] for entry in data if \"loss\" in entry]\n",
    "print(data[0][\"loss\"])\n",
    "losses  = [entry[\"loss\"] for entry in data if \"loss\" in entry]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(steps,losses,label='TrainingLoss')\n",
    "plt.xlabel('Step', weight=\"bold\")\n",
    "plt.ylabel('Training Loss',weight=\"bold\")\n",
    "plt.title('fine-tuning on novel TPCDS-dataset',weight=\"bold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('pre-training_loss_plt.pdf',format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8fcca2d6004254a3c3a0476e19d835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262a583918764124b4d5259870c17360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#INFERENCE MODE -finetuned model\n",
    "\n",
    "#combine base and pretrained model - since pre-training using LORA is like adding new weights to the base model, that is why you need to merge the adapter and base model\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model_id = \"../models/7B/output\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "new_model_id = \"llama-2-7b-finetuned-text2SQL\"\n",
    "new_model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, new_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True,device=\"cuda\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SYSTEM]:\"You are an expert Text-to-SQL generator assistant. Your goal is to provide correct SQL queries to the given text description. Your output only contains the SQL code. No explanation or introductory sentences surrounding the SQL response is needed. You are given schema information. Here is the schema information: \n",
      "<tableName>web_sales</tableName>\n",
      "<columns>ws_sold_date_sk,  ws_sold_time_sk,  ws_ship_date_sk,  ws_item_sk,  ws_bill_customer_sk,  ws_bill_cdemo_sk,  ws_bill_hdemo_sk,  ws_bill_addr_sk,  ws_ship_customer_sk,  ws_ship_cdemo_sk,  ws_ship_hdemo_sk,  ws_ship_addr_sk,  ws_web_page_sk,  ws_web_site_sk,  ws_ship_mode_sk,  ws_warehouse_sk,  ws_promo_sk,  ws_order_number,  ws_quantity,  ws_wholesale_cost,  ws_list_price,  ws_sales_price,  ws_ext_discount_amt,  ws_ext_sales_price,  ws_ext_wholesale_cost,  ws_ext_list_price,  ws_ext_tax,  ws_coupon_amt,  ws_ext_ship_cost,  ws_net_paid,  ws_net_paid_inc_tax,  ws_net_paid_inc_ship,  ws_net_paid_inc_ship_tax,  ws_net_profit</columns>\n",
      "<tableName> web_returns</tableName>\n",
      "<columns>wr_returned_date_sk,  wr_returned_time_sk,  wr_item_sk,  wr_refunded_customer_sk,  wr_refunded_cdemo_sk,  wr_refunded_hdemo_sk,  wr_refunded_addr_sk,  wr_returning_customer_sk,  wr_returning_cdemo_sk,  wr_returning_hdemo_sk,  wr_returning_addr_sk,  wr_web_page_sk,  wr_reason_sk,  wr_order_number,  wr_return_quantity,  wr_return_amt,  wr_return_tax,  wr_return_amt_inc_tax,  wr_fee,  wr_return_ship_cost,  wr_refunded_cash,  wr_reversed_charge,  wr_account_credit,  wr_net_loss</columns>\n",
      "<tableName> web_page</tableName>\n",
      "<columns>wp_web_page_sk,  wp_web_page_id,  wp_rec_start_date,  wp_rec_end_date,  wp_creation_date_sk,  wp_access_date_sk,  wp_autogen_flag,  wp_customer_sk,  wp_url,  wp_type,  wp_char_count,  wp_link_count,  wp_image_count,  wp_max_ad_count</columns>\n",
      "<tableName> customer_demographics</tableName>\n",
      "<columns>cd_demo_sk,  cd_gender,  cd_marital_status,  cd_education_status,  cd_purchase_estimate,  cd_credit_rating,  cd_dep_count,  cd_dep_employed_count,  cd_dep_college_count</columns>\n",
      "<tableName> customer_address</tableName>\n",
      "<columns>ca_address_sk,  ca_address_id,  ca_street_number,  ca_street_name,  ca_street_type,  ca_suite_number,  ca_city,  ca_county,  ca_state,  ca_zip,  ca_country,  ca_gmt_offset,  ca_location_type</columns>\n",
      "<tableName> date_dim</tableName>\n",
      "<columns>d_date_sk,  d_date_id,  d_date,  d_month_seq,  d_week_seq,  d_quarter_seq,  d_year,  d_dow,  d_moy,  d_dom,  d_qoy,  d_fy_year,  d_fy_quarter_seq,  d_fy_week_seq,  d_day_name,  d_quarter_name,  d_holiday,  d_weekend,  d_following_holiday,  d_first_dom,  d_last_dom,  d_same_day_ly,  d_same_day_lq,  d_current_day,  d_current_week,  d_current_month,  d_current_quarter,  d_current_year</columns>\n",
      "<tableName> reason</tableName>\n",
      "<columns>r_reason_sk,  r_reason_id,  r_reason_desc</columns>\n",
      ". Here are the 5 critical rules for the interactions you must abide: <rules> 1. Do not wrap the generated SQL code within SQL code markdown format. Also, do not include the SQL keyword in the beginning of the response. 2. If I don't tell you to find the limited set of results, limit to 100. 3. Only use table and columns from the list provided 4. When performing aliasing, make sure to refer the aliased tables as alias.column_name and not as alias_column_name. 5. For US state names, use abbreviated forms. For example, for South Dakota state, use SD.</rules> \n",
      "\n",
      "\"Here is the user question:\"[/SYSTEM]\n",
      "[HUMAN]: Calculate the average sales, average refunded cash, and average return fee for all web return reasons. The calculations are based on different combinations of customer and sales types, such as marital status, education status, state, and sales profit.\n",
      "[/HUMAN]\n",
      "\n",
      "[SEP] SELECT AVG(ws_sold_date_sk), AVG(ws_sold_time_sk), AVG(ws_ship_date_sk), AVG(ws_item_sk), AVG(ws_bill_customer_sk), AVG(ws_bill_cdemo_sk), AVG(ws_bill_hdemo_sk), AVG(ws_bill_addr_sk), AVG(ws_ship_customer_sk), AVG(ws_ship_cdemo_sk), AVG(ws_ship_hdemo_sk), AVG(ws_ship_addr_sk), AVG(ws_web_page_sk), AVG(ws_web_site_sk), AVG(ws_ship_mode_sk), AVG(ws_warehouse_sk), AVG(ws_promo_sk), AVG(ws_order_number), AVG(ws_quantity), AVG(ws_wholesale_cost), AVG(ws_list_price), AVG(ws_sales_price), AVG(ws_ext_discount_amt), AVG(ws_ext_sales_price), AVG(ws_ext_wholesale_cost), AVG(ws_ext_list_price), AVG(ws_ext_tax), AVG(ws_coupon_amt), AVG(ws_ext_ship_cost), AVG(ws_net_paid), AVG(ws_net_paid_inc_tax), AVG(ws_net_paid_inc_ship), AVG(ws_net_paid_inc_ship_tax), AVG(ws_net_profit) FROM web_sales WHERE ws_sold_date_sk <> 0 AND ws_sold_time_sk <> 0 AND ws_ship_date_sk <> 0 AND ws_item_sk <> 0 AND ws_bill_customer_sk <> 0 AND ws_bill_cdemo_sk <> 0 AND ws_bill_hdemo_sk <> 0 AND ws_bill_addr_sk <> 0 AND ws_ship_customer_sk <> 0 AND ws_ship_cdemo_sk <> 0 AND ws_ship_hdemo_sk <> 0 AND ws_ship_addr_sk <> 0 AND ws_web_page_sk <> 0 AND ws_web_site_sk <> 0 AND ws_ship_mode_sk <> 0 AND ws_warehouse_sk <> 0 AND ws_promo_sk <> 0 AND ws_order_number <> 0 AND ws_quantity <> 0 AND ws_wholesale_cost <> 0 AND ws_list_price <> 0 AND ws_sales_price <> 0 AND ws_ext_discount_amt <> 0 AND ws_ext_sales_price <> 0 AND ws_ext_wholesale_cost <> 0 AND ws_ext_list_price <> 0 AND ws_ext_tax <> 0 AND ws_coupon_amt <> 0 AND ws_ext_ship_cost <> 0 AND ws_net_paid <> 0 AND ws_net_paid_inc_tax <> 0 AND ws_net_paid_inc_ship <> 0 AND ws_net_paid_inc_ship_tax <> 0 AND ws_net_profit <> 0 AND ws_return_quantity <> 0 AND ws_return_amt <> 0 AND ws_return_tax <> 0 AND ws_return_amt_inc_tax <> 0 AND ws_return_ship_cost <> 0 AND ws_reversed_charge <> 0 AND ws_account_credit <> 0 AND ws_net_loss <> 0 AND r_reason_sk <> 0 AND cd_demo_sk <> 0 AND cd_gender <> 0 AND cd_marital_status <> 0 AND cd_education_status <> 0 AND cd_purchase_estimate <> 0 AND cd_credit_rating <> 0 AND cd_dep_count <> 0 AND cd_dep_employed_count <> 0 AND cd_dep_college_count <> 0 AND ca_address_sk <> 0 AND ca_address_id <> 0 AND ca_street_number <> 0 AND ca_street_name <> 0 AND ca_street_type <> 0 AND ca_suite_number <> 0 AND ca_city <> 0 AND ca_county <> 0 AND ca_state <> 0 AND ca_zip <> 0 AND ca_country <> 0 AND ca_gmt_offset <> 0 AND ca_location_type <> 0 AND d_date_sk <> 0 AND d_date_id <> 0 AND d_date <> 0 AND d_month_seq <> 0 AND d_week_seq <> 0 AND d_quarter_seq <> 0 AND d_year <> 0 AND d_dow <> 0 AND d_moy <> 0 AND d_dom <> 0 AND d_qoy <> 0 AND d_fy_year <> 0 AND d_fy_quarter_seq <> 0 AND d_fy_week_seq <> 0 AND d_day_name <> 0 AND d_quarter_name <> 0 AND d_holiday <> 0 AND d_weekend <> 0 AND d_following_holiday <> 0 AND d_first_dom <> 0 AND d_last_dom <> 0 AND d_same_day_ly <> 0 AND d_same_day_lq <> 0 AND d_current_day <> 0 AND d_current_week <> 0 AND d_current_month <> 0 AND d_current_quarter <> 0 AND d_current_year <> 0 AND d_fy_quarter_seq <> 0 AND d_fy_week_seq <> 0 AND d_day_name <> 0 AND d_quarter_name <> 0 AND d_holiday <> 0 AND d_weekend <> 0 AND d_following_holiday <> 0 AND d_first_dom <> 0 AND d_last_dom <> 0 AND d_same_day_ly <> 0 AND d_same_day_lq <> 0 AND d_current_day <> 0 AND d_current_week <> 0 AND d_current_month <> 0 AND d_current_quarter <> 0 AND d_current_year <> 0 AND r_reason_id <> 0 AND r_reason_desc <> 0 AND d_fy_year <> 0 AND d_fy_quarter_seq <> 0 AND d_fy_week_seq <> 0 AND d_day_name <> 0 AND d_quarter_name <> 0 AND d_holiday <> 0 AND d_weekend <> 0 AND d_following_holiday <> 0 AND d_first_dom <> 0 AND d_last_dom <> 0 AND d_same_day_ly <> 0 AND d_same_day_lq <> 0 AND d_current_day <> 0 AND d_current_week <> 0 AND d_current_month <> 0 AND d_current_quarter <> 0 AND d_current_year <> 0 AND d_fy_quarter_seq <> 0 AND d_fy_week_seq <> 0 AND d_day_name <> 0 AND d_quarter_name <> 0 AND d_holiday <> 0 AND d_weekend <> 0 AND d_following_holiday <> 0 AND d_first_dom <> 0 AND d_last_dom <> 0 AND d_same_day_ly <> 0 AND d_same_day_lq <> 0 AND d_current_day <> 0 AND d_current_week <> 0 AND d_current_month <> 0 AND d_current_quarter <> 0 AND d_current_year <> 0 AND r_reason_id <> 0 AND r_reason_desc <> 0 AND d_fy_year <> 0 AND d_fy_quarter_seq <> 0 AND d_fy_week_seq <> 0 AND d_day_name <> 0 AND d_quarter_name <> 0 AND d_holiday <> 0 AND d_weekend <> 0 AND d_following_holiday <> 0 AND d_first_dom <> 0 AND d_last_dom <> 0 AND d_same_day_ly <> 0 AND d_same_day_lq <> 0 AND d_current_day <> 0 AND d_current_week <> 0 AND d_current_month <> 0 AND d_current_quarter <> 0 AND d_current_year <> 0 AND r_reason_id <> 0 AND r_reason_desc <> 0 AND d_fy_year <> 0 AND d_fy_quarter_\n"
     ]
    }
   ],
   "source": [
    "#sample inferene running\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config,\n",
    "      \n",
    "      \n",
    "    \n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = base_model.generation_config\n",
    "generation_config.max_new_tokens = 2048\n",
    "#generation_config.temperature = 0.7\n",
    "#generation_config.top_p = 0.7\n",
    "#generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started infernece for 1\n",
      "inference finished for query1\n",
      "completed writing response ot a file\n",
      "started infernece for 2\n",
      "inference finished for query2\n",
      "completed writing response ot a file\n",
      "started infernece for 3\n",
      "inference finished for query3\n",
      "completed writing response ot a file\n",
      "started infernece for 4\n",
      "inference finished for query4\n",
      "completed writing response ot a file\n",
      "started infernece for 5\n",
      "inference finished for query5\n",
      "completed writing response ot a file\n",
      "started infernece for 6\n",
      "inference finished for query6\n",
      "completed writing response ot a file\n",
      "started infernece for 7\n",
      "inference finished for query7\n",
      "completed writing response ot a file\n",
      "started infernece for 8\n",
      "inference finished for query8\n",
      "completed writing response ot a file\n",
      "started infernece for 9\n",
      "inference finished for query9\n",
      "completed writing response ot a file\n",
      "started infernece for 10\n",
      "inference finished for query10\n",
      "completed writing response ot a file\n",
      "started infernece for 11\n",
      "inference finished for query11\n",
      "completed writing response ot a file\n",
      "started infernece for 12\n",
      "inference finished for query12\n",
      "completed writing response ot a file\n",
      "started infernece for 13\n",
      "inference finished for query13\n",
      "completed writing response ot a file\n",
      "started infernece for 14\n",
      "inference finished for query14\n",
      "completed writing response ot a file\n",
      "started infernece for 15\n",
      "inference finished for query15\n",
      "completed writing response ot a file\n",
      "started infernece for 16\n",
      "inference finished for query16\n",
      "completed writing response ot a file\n",
      "started infernece for 17\n",
      "inference finished for query17\n",
      "completed writing response ot a file\n",
      "started infernece for 18\n",
      "inference finished for query18\n",
      "completed writing response ot a file\n",
      "started infernece for 19\n",
      "inference finished for query19\n",
      "completed writing response ot a file\n",
      "started infernece for 20\n",
      "inference finished for query20\n",
      "completed writing response ot a file\n",
      "started infernece for 21\n",
      "inference finished for query21\n",
      "completed writing response ot a file\n",
      "started infernece for 22\n",
      "inference finished for query22\n",
      "completed writing response ot a file\n",
      "started infernece for 23\n",
      "inference finished for query23\n",
      "completed writing response ot a file\n",
      "started infernece for 24\n",
      "inference finished for query24\n",
      "completed writing response ot a file\n",
      "started infernece for 25\n",
      "inference finished for query25\n",
      "completed writing response ot a file\n",
      "started infernece for 26\n",
      "inference finished for query26\n",
      "completed writing response ot a file\n",
      "started infernece for 27\n",
      "inference finished for query27\n",
      "completed writing response ot a file\n",
      "started infernece for 28\n",
      "inference finished for query28\n",
      "completed writing response ot a file\n",
      "started infernece for 29\n",
      "inference finished for query29\n",
      "completed writing response ot a file\n",
      "started infernece for 30\n",
      "inference finished for query30\n",
      "completed writing response ot a file\n",
      "started infernece for 31\n",
      "inference finished for query31\n",
      "completed writing response ot a file\n",
      "started infernece for 32\n",
      "inference finished for query32\n",
      "completed writing response ot a file\n",
      "started infernece for 33\n",
      "inference finished for query33\n",
      "completed writing response ot a file\n",
      "started infernece for 34\n",
      "inference finished for query34\n",
      "completed writing response ot a file\n",
      "started infernece for 35\n",
      "inference finished for query35\n",
      "completed writing response ot a file\n",
      "started infernece for 36\n",
      "inference finished for query36\n",
      "completed writing response ot a file\n",
      "started infernece for 37\n",
      "inference finished for query37\n",
      "completed writing response ot a file\n",
      "started infernece for 38\n",
      "inference finished for query38\n",
      "completed writing response ot a file\n",
      "started infernece for 39\n",
      "inference finished for query39\n",
      "completed writing response ot a file\n",
      "started infernece for 40\n",
      "inference finished for query40\n",
      "completed writing response ot a file\n",
      "started infernece for 41\n",
      "inference finished for query41\n",
      "completed writing response ot a file\n",
      "started infernece for 42\n",
      "inference finished for query42\n",
      "completed writing response ot a file\n",
      "started infernece for 43\n",
      "inference finished for query43\n",
      "completed writing response ot a file\n",
      "started infernece for 44\n",
      "inference finished for query44\n",
      "completed writing response ot a file\n",
      "started infernece for 45\n",
      "inference finished for query45\n",
      "completed writing response ot a file\n",
      "started infernece for 46\n",
      "inference finished for query46\n",
      "completed writing response ot a file\n",
      "started infernece for 47\n",
      "inference finished for query47\n",
      "completed writing response ot a file\n",
      "started infernece for 48\n",
      "inference finished for query48\n",
      "completed writing response ot a file\n",
      "started infernece for 49\n",
      "inference finished for query49\n",
      "completed writing response ot a file\n",
      "started infernece for 50\n",
      "inference finished for query50\n",
      "completed writing response ot a file\n",
      "started infernece for 51\n",
      "inference finished for query51\n",
      "completed writing response ot a file\n",
      "started infernece for 52\n",
      "inference finished for query52\n",
      "completed writing response ot a file\n",
      "started infernece for 53\n",
      "inference finished for query53\n",
      "completed writing response ot a file\n",
      "started infernece for 54\n",
      "inference finished for query54\n",
      "completed writing response ot a file\n",
      "started infernece for 55\n",
      "inference finished for query55\n",
      "completed writing response ot a file\n",
      "started infernece for 56\n",
      "inference finished for query56\n",
      "completed writing response ot a file\n",
      "started infernece for 57\n",
      "inference finished for query57\n",
      "completed writing response ot a file\n",
      "started infernece for 58\n",
      "inference finished for query58\n",
      "completed writing response ot a file\n",
      "started infernece for 59\n",
      "inference finished for query59\n",
      "completed writing response ot a file\n",
      "started infernece for 60\n",
      "inference finished for query60\n",
      "completed writing response ot a file\n",
      "started infernece for 61\n",
      "inference finished for query61\n",
      "completed writing response ot a file\n",
      "started infernece for 62\n",
      "inference finished for query62\n",
      "completed writing response ot a file\n",
      "started infernece for 63\n",
      "inference finished for query63\n",
      "completed writing response ot a file\n",
      "started infernece for 64\n",
      "inference finished for query64\n",
      "completed writing response ot a file\n",
      "started infernece for 65\n",
      "inference finished for query65\n",
      "completed writing response ot a file\n",
      "started infernece for 66\n",
      "inference finished for query66\n",
      "completed writing response ot a file\n",
      "started infernece for 67\n",
      "inference finished for query67\n",
      "completed writing response ot a file\n",
      "started infernece for 68\n",
      "inference finished for query68\n",
      "completed writing response ot a file\n",
      "started infernece for 69\n",
      "inference finished for query69\n",
      "completed writing response ot a file\n",
      "started infernece for 70\n",
      "inference finished for query70\n",
      "completed writing response ot a file\n",
      "started infernece for 71\n",
      "inference finished for query71\n",
      "completed writing response ot a file\n",
      "started infernece for 72\n",
      "inference finished for query72\n",
      "completed writing response ot a file\n",
      "started infernece for 73\n",
      "inference finished for query73\n",
      "completed writing response ot a file\n",
      "started infernece for 74\n",
      "inference finished for query74\n",
      "completed writing response ot a file\n",
      "started infernece for 75\n",
      "inference finished for query75\n",
      "completed writing response ot a file\n",
      "started infernece for 76\n",
      "inference finished for query76\n",
      "completed writing response ot a file\n",
      "started infernece for 77\n",
      "inference finished for query77\n",
      "completed writing response ot a file\n",
      "started infernece for 78\n",
      "inference finished for query78\n",
      "completed writing response ot a file\n",
      "started infernece for 79\n",
      "inference finished for query79\n",
      "completed writing response ot a file\n",
      "started infernece for 80\n",
      "inference finished for query80\n",
      "completed writing response ot a file\n",
      "started infernece for 81\n",
      "inference finished for query81\n",
      "completed writing response ot a file\n",
      "started infernece for 82\n",
      "inference finished for query82\n",
      "completed writing response ot a file\n",
      "started infernece for 83\n",
      "inference finished for query83\n",
      "completed writing response ot a file\n",
      "started infernece for 84\n",
      "inference finished for query84\n",
      "completed writing response ot a file\n",
      "started infernece for 85\n",
      "inference finished for query85\n",
      "completed writing response ot a file\n",
      "started infernece for 86\n",
      "inference finished for query86\n",
      "completed writing response ot a file\n",
      "started infernece for 87\n",
      "inference finished for query87\n",
      "completed writing response ot a file\n",
      "started infernece for 88\n",
      "inference finished for query88\n",
      "completed writing response ot a file\n",
      "started infernece for 89\n",
      "inference finished for query89\n",
      "completed writing response ot a file\n",
      "started infernece for 90\n",
      "inference finished for query90\n",
      "completed writing response ot a file\n",
      "started infernece for 91\n",
      "inference finished for query91\n",
      "completed writing response ot a file\n",
      "started infernece for 92\n",
      "inference finished for query92\n",
      "completed writing response ot a file\n",
      "started infernece for 93\n",
      "inference finished for query93\n",
      "completed writing response ot a file\n",
      "started infernece for 94\n",
      "inference finished for query94\n",
      "completed writing response ot a file\n",
      "started infernece for 95\n",
      "inference finished for query95\n",
      "completed writing response ot a file\n",
      "started infernece for 96\n",
      "inference finished for query96\n",
      "completed writing response ot a file\n",
      "started infernece for 97\n",
      "inference finished for query97\n",
      "completed writing response ot a file\n",
      "started infernece for 98\n",
      "inference finished for query98\n",
      "completed writing response ot a file\n",
      "started infernece for 99\n",
      "inference finished for query99\n",
      "completed writing response ot a file\n",
      "started infernece for 100\n",
      "inference finished for query100\n",
      "completed writing response ot a file\n",
      "started infernece for 101\n",
      "inference finished for query101\n",
      "completed writing response ot a file\n",
      "started infernece for 102\n",
      "inference finished for query102\n",
      "completed writing response ot a file\n",
      "started infernece for 103\n",
      "inference finished for query103\n",
      "completed writing response ot a file\n",
      "started infernece for 104\n",
      "inference finished for query104\n",
      "completed writing response ot a file\n",
      "started infernece for 105\n",
      "inference finished for query105\n",
      "completed writing response ot a file\n",
      "started infernece for 106\n",
      "inference finished for query106\n",
      "completed writing response ot a file\n",
      "started infernece for 107\n",
      "inference finished for query107\n",
      "completed writing response ot a file\n",
      "started infernece for 108\n",
      "inference finished for query108\n",
      "completed writing response ot a file\n",
      "started infernece for 109\n",
      "inference finished for query109\n",
      "completed writing response ot a file\n",
      "started infernece for 110\n",
      "inference finished for query110\n",
      "completed writing response ot a file\n",
      "started infernece for 111\n",
      "inference finished for query111\n",
      "completed writing response ot a file\n",
      "started infernece for 112\n",
      "inference finished for query112\n",
      "completed writing response ot a file\n",
      "started infernece for 113\n",
      "inference finished for query113\n",
      "completed writing response ot a file\n",
      "started infernece for 114\n",
      "inference finished for query114\n",
      "completed writing response ot a file\n",
      "started infernece for 115\n",
      "inference finished for query115\n",
      "completed writing response ot a file\n",
      "started infernece for 116\n",
      "inference finished for query116\n",
      "completed writing response ot a file\n",
      "started infernece for 117\n",
      "inference finished for query117\n",
      "completed writing response ot a file\n",
      "started infernece for 118\n",
      "inference finished for query118\n",
      "completed writing response ot a file\n",
      "started infernece for 119\n",
      "inference finished for query119\n",
      "completed writing response ot a file\n",
      "started infernece for 120\n",
      "inference finished for query120\n",
      "completed writing response ot a file\n",
      "started infernece for 121\n",
      "inference finished for query121\n",
      "completed writing response ot a file\n",
      "started infernece for 122\n",
      "inference finished for query122\n",
      "completed writing response ot a file\n",
      "started infernece for 123\n",
      "inference finished for query123\n",
      "completed writing response ot a file\n",
      "started infernece for 124\n",
      "inference finished for query124\n",
      "completed writing response ot a file\n",
      "started infernece for 125\n",
      "inference finished for query125\n",
      "completed writing response ot a file\n",
      "started infernece for 126\n",
      "inference finished for query126\n",
      "completed writing response ot a file\n",
      "started infernece for 127\n",
      "inference finished for query127\n",
      "completed writing response ot a file\n",
      "started infernece for 128\n",
      "inference finished for query128\n",
      "completed writing response ot a file\n",
      "started infernece for 129\n",
      "inference finished for query129\n",
      "completed writing response ot a file\n",
      "started infernece for 130\n",
      "inference finished for query130\n",
      "completed writing response ot a file\n",
      "started infernece for 131\n",
      "inference finished for query131\n",
      "completed writing response ot a file\n",
      "started infernece for 132\n",
      "inference finished for query132\n",
      "completed writing response ot a file\n",
      "started infernece for 133\n",
      "inference finished for query133\n",
      "completed writing response ot a file\n",
      "started infernece for 134\n",
      "inference finished for query134\n",
      "completed writing response ot a file\n",
      "started infernece for 135\n",
      "inference finished for query135\n",
      "completed writing response ot a file\n",
      "started infernece for 136\n",
      "inference finished for query136\n",
      "completed writing response ot a file\n",
      "started infernece for 137\n",
      "inference finished for query137\n",
      "completed writing response ot a file\n",
      "started infernece for 138\n",
      "inference finished for query138\n",
      "completed writing response ot a file\n",
      "started infernece for 139\n",
      "inference finished for query139\n",
      "completed writing response ot a file\n",
      "started infernece for 140\n",
      "inference finished for query140\n",
      "completed writing response ot a file\n",
      "started infernece for 141\n",
      "inference finished for query141\n",
      "completed writing response ot a file\n",
      "started infernece for 142\n",
      "inference finished for query142\n",
      "completed writing response ot a file\n",
      "started infernece for 143\n",
      "inference finished for query143\n",
      "completed writing response ot a file\n",
      "started infernece for 144\n",
      "inference finished for query144\n",
      "completed writing response ot a file\n",
      "started infernece for 145\n",
      "inference finished for query145\n",
      "completed writing response ot a file\n",
      "started infernece for 146\n",
      "inference finished for query146\n",
      "completed writing response ot a file\n",
      "started infernece for 147\n",
      "inference finished for query147\n",
      "completed writing response ot a file\n",
      "started infernece for 148\n",
      "inference finished for query148\n",
      "completed writing response ot a file\n",
      "started infernece for 149\n",
      "inference finished for query149\n",
      "completed writing response ot a file\n",
      "started infernece for 150\n",
      "inference finished for query150\n",
      "completed writing response ot a file\n",
      "started infernece for 151\n",
      "inference finished for query151\n",
      "completed writing response ot a file\n",
      "started infernece for 152\n",
      "inference finished for query152\n",
      "completed writing response ot a file\n",
      "started infernece for 153\n",
      "inference finished for query153\n",
      "completed writing response ot a file\n",
      "started infernece for 154\n",
      "inference finished for query154\n",
      "completed writing response ot a file\n",
      "started infernece for 155\n",
      "inference finished for query155\n",
      "completed writing response ot a file\n",
      "started infernece for 156\n",
      "inference finished for query156\n",
      "completed writing response ot a file\n",
      "started infernece for 157\n",
      "inference finished for query157\n",
      "completed writing response ot a file\n",
      "started infernece for 158\n",
      "inference finished for query158\n",
      "completed writing response ot a file\n",
      "started infernece for 159\n",
      "inference finished for query159\n",
      "completed writing response ot a file\n",
      "started infernece for 160\n",
      "inference finished for query160\n",
      "completed writing response ot a file\n",
      "started infernece for 161\n",
      "inference finished for query161\n",
      "completed writing response ot a file\n",
      "started infernece for 162\n",
      "inference finished for query162\n",
      "completed writing response ot a file\n",
      "started infernece for 163\n",
      "inference finished for query163\n",
      "completed writing response ot a file\n",
      "started infernece for 164\n",
      "inference finished for query164\n",
      "completed writing response ot a file\n",
      "started infernece for 165\n",
      "inference finished for query165\n",
      "completed writing response ot a file\n",
      "started infernece for 166\n",
      "inference finished for query166\n",
      "completed writing response ot a file\n",
      "started infernece for 167\n",
      "inference finished for query167\n",
      "completed writing response ot a file\n",
      "started infernece for 168\n",
      "inference finished for query168\n",
      "completed writing response ot a file\n",
      "started infernece for 169\n",
      "inference finished for query169\n",
      "completed writing response ot a file\n",
      "started infernece for 170\n",
      "inference finished for query170\n",
      "completed writing response ot a file\n",
      "started infernece for 171\n",
      "inference finished for query171\n",
      "completed writing response ot a file\n",
      "started infernece for 172\n",
      "inference finished for query172\n",
      "completed writing response ot a file\n",
      "started infernece for 173\n",
      "inference finished for query173\n",
      "completed writing response ot a file\n",
      "started infernece for 174\n",
      "inference finished for query174\n",
      "completed writing response ot a file\n",
      "started infernece for 175\n",
      "inference finished for query175\n",
      "completed writing response ot a file\n",
      "started infernece for 176\n",
      "inference finished for query176\n",
      "completed writing response ot a file\n",
      "started infernece for 177\n",
      "inference finished for query177\n",
      "completed writing response ot a file\n",
      "started infernece for 178\n",
      "inference finished for query178\n",
      "completed writing response ot a file\n",
      "started infernece for 179\n",
      "inference finished for query179\n",
      "completed writing response ot a file\n",
      "started infernece for 180\n",
      "inference finished for query180\n",
      "completed writing response ot a file\n",
      "started infernece for 181\n",
      "inference finished for query181\n",
      "completed writing response ot a file\n",
      "started infernece for 182\n",
      "inference finished for query182\n",
      "completed writing response ot a file\n",
      "started infernece for 183\n",
      "inference finished for query183\n",
      "completed writing response ot a file\n",
      "started infernece for 184\n",
      "inference finished for query184\n",
      "completed writing response ot a file\n",
      "started infernece for 185\n",
      "inference finished for query185\n",
      "completed writing response ot a file\n",
      "started infernece for 186\n",
      "inference finished for query186\n",
      "completed writing response ot a file\n",
      "started infernece for 187\n",
      "inference finished for query187\n",
      "completed writing response ot a file\n",
      "started infernece for 188\n",
      "inference finished for query188\n",
      "completed writing response ot a file\n",
      "started infernece for 189\n",
      "inference finished for query189\n",
      "completed writing response ot a file\n",
      "started infernece for 190\n",
      "inference finished for query190\n",
      "completed writing response ot a file\n",
      "started infernece for 191\n",
      "inference finished for query191\n",
      "completed writing response ot a file\n",
      "started infernece for 192\n",
      "inference finished for query192\n",
      "completed writing response ot a file\n",
      "started infernece for 193\n",
      "inference finished for query193\n",
      "completed writing response ot a file\n",
      "started infernece for 194\n",
      "inference finished for query194\n",
      "completed writing response ot a file\n",
      "started infernece for 195\n",
      "inference finished for query195\n",
      "completed writing response ot a file\n",
      "started infernece for 196\n",
      "inference finished for query196\n",
      "completed writing response ot a file\n",
      "started infernece for 197\n",
      "inference finished for query197\n",
      "completed writing response ot a file\n",
      "started infernece for 198\n",
      "inference finished for query198\n",
      "completed writing response ot a file\n",
      "started infernece for 199\n",
      "inference finished for query199\n",
      "completed writing response ot a file\n",
      "started infernece for 200\n",
      "inference finished for query200\n",
      "completed writing response ot a file\n",
      "started infernece for 201\n",
      "inference finished for query201\n",
      "completed writing response ot a file\n",
      "started infernece for 202\n",
      "inference finished for query202\n",
      "completed writing response ot a file\n",
      "started infernece for 203\n",
      "inference finished for query203\n",
      "completed writing response ot a file\n",
      "started infernece for 204\n",
      "inference finished for query204\n",
      "completed writing response ot a file\n",
      "started infernece for 205\n",
      "inference finished for query205\n",
      "completed writing response ot a file\n",
      "started infernece for 206\n",
      "inference finished for query206\n",
      "completed writing response ot a file\n",
      "started infernece for 207\n",
      "inference finished for query207\n",
      "completed writing response ot a file\n",
      "started infernece for 208\n",
      "inference finished for query208\n",
      "completed writing response ot a file\n",
      "started infernece for 209\n",
      "inference finished for query209\n",
      "completed writing response ot a file\n",
      "started infernece for 210\n",
      "inference finished for query210\n",
      "completed writing response ot a file\n",
      "started infernece for 211\n",
      "inference finished for query211\n",
      "completed writing response ot a file\n",
      "started infernece for 212\n",
      "inference finished for query212\n",
      "completed writing response ot a file\n",
      "started infernece for 213\n",
      "inference finished for query213\n",
      "completed writing response ot a file\n",
      "started infernece for 214\n",
      "inference finished for query214\n",
      "completed writing response ot a file\n",
      "started infernece for 215\n",
      "inference finished for query215\n",
      "completed writing response ot a file\n",
      "started infernece for 216\n",
      "inference finished for query216\n",
      "completed writing response ot a file\n",
      "started infernece for 217\n",
      "inference finished for query217\n",
      "completed writing response ot a file\n"
     ]
    }
   ],
   "source": [
    "#generating responses for all test dataset\n",
    "import os\n",
    "import time\n",
    "query_time=\"\"\n",
    "for queryIdx in range(len(test_splits)):\n",
    "    start=time.time()\n",
    "    print(f\"started infernece for {queryIdx+1}\")\n",
    "    with torch.inference_mode():\n",
    "        encoding = tokenizer(test_splits[queryIdx][\"text\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(input_ids = encoding.input_ids, attention_mask = encoding.attention_mask,generation_config = generation_config) \n",
    "        query_time+= str(time.time() - start)  +\"\\n\"\n",
    "        print(f\"inference finished for query{queryIdx+1}\")\n",
    "        with open(f\"./test-results/llm-results/query_{queryIdx+1}.txt\",\"w\") as response_file:\n",
    "            response_file.write(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "            response_file.close()\n",
    "        print(\"completed writing response ot a file\")\n",
    "\n",
    "with open(f\"./test-results/querytime/query-time.txt\",\"w\") as time_file:\n",
    "            time_file.write(query_time)\n",
    "            time_file.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files successfully copied to ./test-results/sampled-queries\n"
     ]
    }
   ],
   "source": [
    "#this code randomly selects 99 queries from llama results (random sampling) to execute on db\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Paths for source and destination folders\n",
    "src_folder = './test-results/llm-responses'\n",
    "dest_folder = './test-results/sampled-queries'\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "# Generate a set of unique random numbers\n",
    "random_numbers = set(random.sample(range(1, 218), k=99))  # Adjust 'k' for the number of files you want to copy\n",
    "\n",
    "# Copy files based on generated numbers\n",
    "for number in random_numbers:\n",
    "    file_name = f'query_{number}.txt'\n",
    "    src_path = os.path.join(src_folder, file_name)\n",
    "    dest_path = os.path.join(dest_folder, file_name)\n",
    "\n",
    "    # Copy the file from the source to the destination\n",
    "    shutil.copy(src_path, dest_path)\n",
    "\n",
    "print(f\"Files successfully copied to {dest_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30db67269510411db47a63de087c9e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997455e9a0754607aaab9ede6710c6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6100b072c25c457ca997aa1eee6f6ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d883c49d658948548161c627c3f59e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2be593ad5545049a50af39c343561c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of TPCDS DB complete.\n"
     ]
    }
   ],
   "source": [
    "#execute randomly sampled 99 queries on duckdb\n",
    "\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the connection parameters\n",
    "query_path='./test-results/sampled-queries/'\n",
    "query_result='./test-results/db-results/llm-resp-db-results/results/'\n",
    "query_time_file='./test-results/db-results/llm-resp-db-results/querytime/log.txt'\n",
    "\n",
    "# Connect to the DB \n",
    "db_con = duckdb.connect()\n",
    "\n",
    "# Load the TPCDS database\n",
    "db_con.execute(\"IMPORT DATABASE '/workspace/data/duckdb/build/release/tpcdssf100'\")\n",
    "\n",
    "print(\"Loading of TPCDS DB complete.\")\n",
    "\n",
    "# Create a folder to store query results\n",
    "os.makedirs(query_result, exist_ok=True)\n",
    "\n",
    "query_files = [f for f in os.listdir(query_path) if os.path.isfile(os.path.join(query_path, f))]\n",
    "query_files = sorted(query_files)\n",
    "query_exec_time=\"\"\n",
    "# Iterate through the queries and execute them\n",
    "for query_file in query_files:\n",
    "    print(\"Executing \", query_file)\n",
    "    # extract query number from the query file name.\n",
    "    query_name = os.path.splitext(query_file)[0]\n",
    "\n",
    "    # Read the query from the file\n",
    "    with open(os.path.join(query_path, query_file), 'r') as f:\n",
    "        query=f.read()\n",
    "\n",
    "    # Execute the query and measure execution time\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = db_con.execute(query)\n",
    "        execution_time = time.time()-start_time\n",
    "    \n",
    "        # Save the query result to a file\n",
    "        output_file_path = query_result + query_name + \".csv\"\n",
    "        with open(output_file_path, \"w\") as out_file:\n",
    "            # print(\"Writing result:\", out_file)\n",
    "            for row in result.fetchall():\n",
    "                out_file.write(str(row) + \"\\n\")\n",
    "                # print(row)\n",
    "            out_file.close()    \n",
    "    except Exception as e: \n",
    "        print(f'Query {query_name} executed in error: {e}')\n",
    "    query_exec_time+= str(execution_time)+\"\\n\"\n",
    "        \n",
    "with open(query_time_file, \"a\") as time_file:\n",
    "    time_file.write(query_exec_time)\n",
    "\n",
    "    print(f'Query {query_name} executed in {execution_time:.2f} seconds')\n",
    "\n",
    "# Close the database connection\n",
    "db_con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e1bde8b70b4b499c4eacfecbf1dbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f14dbbb0f54092a75f6d459d25ab19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c17a7b2bb1d42e0ad3132255f2e88c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c92d2647c74587b400971695feb3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3cd614fabf457a95bca10af48ea8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of TPCDS DB complete.\n"
     ]
    }
   ],
   "source": [
    "#execute all sampled queries's golden answer on duckdb for comparison\n",
    "\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Define the connection parameters\n",
    "query_path='./test-results/sampled-queries/'\n",
    "query_result='./test-results/db-results/golden-query-db-results/results/'\n",
    "query_time_file='./test-results/db-results/golden-query-db-results/querytime/log.txt'\n",
    "\n",
    "#load test splits as each of the sample has golden truth \n",
    "test_splits=load_from_disk('./dataset-splits/test-split')\n",
    "# Connect to the DB \n",
    "db_con = duckdb.connect()\n",
    "\n",
    "# Load the TPCDS database\n",
    "db_con.execute(\"IMPORT DATABASE '/workspace/data/duckdb/build/release/tpcdssf100'\")\n",
    "\n",
    "print(\"Loading of TPCDS DB complete.\")\n",
    "\n",
    "# Create a folder to store query results\n",
    "os.makedirs(query_result, exist_ok=True)\n",
    "\n",
    "query_files = [f for f in os.listdir(query_path) if os.path.isfile(os.path.join(query_path, f))]\n",
    "query_files = sorted(query_files)\n",
    "query_exec_time=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_10.txt\n",
      "Executing  query_100.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccac178fd434266bf9946ddc0fe0566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_102.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbfd5344bfd4189b96c3c57896ff2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_105.txt\n",
      "Executing  query_109.txt\n",
      "Executing  query_11.txt\n",
      "Executing  query_110.txt\n",
      "Executing  query_113.txt\n",
      "Executing  query_115.txt\n",
      "Executing  query_117.txt\n",
      "Executing  query_119.txt\n",
      "Executing  query_12.txt\n",
      "Executing  query_120.txt\n",
      "Executing  query_124.txt\n",
      "Executing  query_127.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cf879716e843f2b6cb74fd1fca83cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_128.txt\n",
      "Executing  query_130.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d821e725a844a6a9fb44b476cfd313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c2b317aa414c1a923993dce8942c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_133.txt\n",
      "Executing  query_134.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31befbf1bec64bf6b6ee564fb600c209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_138.txt\n",
      "Executing  query_139.txt\n",
      "Executing  query_141.txt\n",
      "Executing  query_146.txt\n",
      "Executing  query_147.txt\n",
      "Executing  query_148.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dafc39f8b34938ac89377bce99ea1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_151.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec081b033b1547d69b4214ee1a2d02e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_153.txt\n",
      "Executing  query_158.txt\n",
      "Executing  query_16.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1e36341c7846d8abe88535730814d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_160.txt\n",
      "Executing  query_162.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc36c7708914496a31b5882e7051967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_163.txt\n",
      "Executing  query_164.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1785aedaec4724b12de3b3725de2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_167.txt\n",
      "Executing  query_169.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a707e3ad779449678fd98ee5f4121500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_17.txt\n",
      "Executing  query_170.txt\n",
      "Executing  query_174.txt\n",
      "Executing  query_175.txt\n",
      "Executing  query_179.txt\n",
      "Executing  query_18.txt\n",
      "Executing  query_182.txt\n",
      "Executing  query_184.txt\n",
      "Executing  query_187.txt\n",
      "Executing  query_188.txt\n",
      "Executing  query_190.txt\n",
      "Executing  query_192.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff0175034a849889db3b29285e8f755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_194.txt\n",
      "Executing  query_197.txt\n",
      "Executing  query_199.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c014640ecacc4c88a80ed811f9414f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_2.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba275adbb8994ed58d0d0533441d67fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28304dac56a4f50bfce1f1192ccb64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_200.txt\n",
      "Executing  query_202.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bbdd50259d43a9bb9caf7cc4a05b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_204.txt\n",
      "Executing  query_206.txt\n",
      "Executing  query_209.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7e99621fee4b4e9c9b9b1fbd08e53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_216.txt\n",
      "Executing  query_22.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf8a9dc49954da9976b1ce6f16f16a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_24.txt\n",
      "Executing  query_31.txt\n",
      "Executing  query_33.txt\n",
      "Executing  query_35.txt\n",
      "Executing  query_36.txt\n",
      "Executing  query_38.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd2f4f0ae224d9fba2fddc86e965817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_4.txt\n",
      "Executing  query_43.txt\n",
      "Executing  query_44.txt\n",
      "Executing  query_47.txt\n",
      "Executing  query_5.txt\n",
      "Executing  query_51.txt\n",
      "Executing  query_52.txt\n",
      "Executing  query_53.txt\n",
      "Executing  query_55.txt\n",
      "Executing  query_58.txt\n",
      "Executing  query_59.txt\n",
      "Executing  query_60.txt\n",
      "Executing  query_64.txt\n",
      "Executing  query_67.txt\n",
      "Executing  query_68.txt\n",
      "Executing  query_72.txt\n",
      "Executing  query_73.txt\n",
      "Executing  query_74.txt\n",
      "Executing  query_77.txt\n",
      "Executing  query_78.txt\n",
      "Executing  query_8.txt\n",
      "Executing  query_80.txt\n",
      "Executing  query_81.txt\n",
      "Executing  query_82.txt\n",
      "Executing  query_83.txt\n",
      "Executing  query_84.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051f32381aff4524b753bb062facfba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_87.txt\n",
      "Executing  query_88.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0522235ada44388befd4a123a80d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_9.txt\n",
      "Executing  query_90.txt\n",
      "Executing  query_92.txt\n",
      "Executing  query_93.txt\n",
      "Executing  query_95.txt\n",
      "Executing  query_97.txt\n",
      "Executing  query_99.txt\n",
      "Query query_99 executed in 0.66 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for query_file in query_files:\n",
    "    print(\"Executing \", query_file)\n",
    "    # extract query number from the query file name.\n",
    "    query_name = os.path.splitext(query_file)[0]\n",
    "    #regex to extract query number from query_10.txt \n",
    "    query_number=int(re.search(r'\\d+',query_name).group())\n",
    "    # print(query_name)\n",
    "    # print(\"query_number\",query_number-1)\n",
    "\n",
    "\n",
    "    sql_query=test_splits[query_number-1][\"output\"]\n",
    "    # print(\"sql_query\",sql_query)\n",
    "    # Execute the query and measure execution time\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = db_con.execute(sql_query)\n",
    "        execution_time = time.time()-start_time\n",
    "    \n",
    "        # Save the query result to a file\n",
    "        output_file_path = query_result + query_name + \".csv\"\n",
    "        with open(output_file_path, \"w\") as out_file:\n",
    "            # print(\"Writing result:\", out_file)\n",
    "            for row in result.fetchall():\n",
    "                out_file.write(str(row) + \"\\n\")\n",
    "                # print(row)\n",
    "            out_file.close()    \n",
    "    except Exception as e: \n",
    "        print(f'Query {query_name} executed in error: {e}')\n",
    "    query_exec_time+= str(execution_time)+\"\\n\"\n",
    "        \n",
    "with open(query_time_file, \"a\") as time_file:\n",
    "    time_file.write(query_exec_time)\n",
    "\n",
    "    print(f'Query {query_name} executed in {execution_time:.2f} seconds')\n",
    "# Close the database connection\n",
    "db_con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'\n",
      "\n",
      "[SYSTEM]:\"You are an expert Text-to-SQL generator assistant. Your goal is to provide correct SQL queries to the given text description. Your output only contains the SQL code. No explanation or introductory sentences surrounding the SQL response is needed. You are given schema information. Here is the schema information: \n",
      "<tableName>store_returns</tableName>\n",
      "<columns>sr_returned_date_sk,  sr_return_time_sk,  sr_item_sk,  sr_customer_sk,  sr_cdemo_sk,  sr_hdemo_sk,  sr_addr_sk,  sr_store_sk,  sr_reason_sk,  sr_ticket_number,  sr_return_quantity,  sr_return_amt,  sr_return_tax,  sr_return_amt_inc_tax,  sr_fee,  sr_return_ship_cost,  sr_refunded_cash,  sr_reversed_charge,  sr_store_credit,  sr_net_loss</columns>\n",
      "<tableName>date_dim</tableName>\n",
      "<columns>d_date_sk,  d_date_id,  d_date,  d_month_seq,  d_week_seq,  d_quarter_seq,  d_year,  d_dow,  d_moy,  d_dom,  d_qoy,  d_fy_year,  d_fy_quarter_seq,  d_fy_week_seq,  d_day_name,  d_quarter_name,  d_holiday,  d_weekend,  d_following_holiday,  d_first_dom,  d_last_dom,  d_same_day_ly,  d_same_day_lq,  d_current_day,  d_current_week,  d_current_month,  d_current_quarter,  d_current_year</columns>\n",
      "<tableName>store</tableName>\n",
      "<columns>s_store_sk,  s_store_id,  s_rec_start_date,  s_rec_end_date,  s_closed_date_sk,  s_store_name,  s_number_employees,  s_floor_space,  s_hours,  s_manager,  s_market_id,  s_geography_class,  s_market_desc,  s_market_manager,  s_division_id,  s_division_name,  s_company_id,  s_company_name,  s_street_number,  s_street_name,  s_street_type,  s_suite_number,  s_city,  s_county,  s_state,  s_zip,  s_country,  s_gmt_offset,  s_tax_percentage</columns>\n",
      "<tableName>customer</tableName>\n",
      "<columns>c_customer_sk,  c_customer_id,  c_current_cdemo_sk,  c_current_hdemo_sk,  c_current_addr_sk,  c_first_shipto_date_sk,  c_first_sales_date_sk,  c_salutation,  c_first_name,  c_last_name,  c_preferred_cust_flag,  c_birth_day,  c_birth_month,  c_birth_year,  c_birth_country,  c_login,  c_email_address,  c_last_review_date_sk</columns>\n",
      ". Here are the 5 critical rules for the interactions you must abide: <rules> 1. Do not wrap the generated SQL code within SQL code markdown format. Also, do not include the SQL keyword in the beginning of the response. 2. If I don't tell you to find the limited set of results, limit to 100. 3. Only use table and columns from the list provided 4. When performing aliasing, make sure to refer the aliased tables as alias.column_name and not as alias_column_name. 5. For US state names, use abbreviated forms. For example, for South Dakota state, use SD.</rules> \n",
      "\n",
      "\"Here is the user question:\"[/SYSTEM]\n",
      "[HUMAN]: For the state of South Dakota in the year 2000, identify the first 100 customers, sorted by their IDs, whose returns are notably higher, exceeding the store's average by more than 20%, indicating a trend of higher returns.\n",
      "[/HUMAN]\n",
      "\n",
      "\n",
      "[SEP] SELECT T1.sr_customer_sk, T1.sr_cdemo_sk, T1.sr_hdemo_sk, T1.sr_return_quantity, T1.sr_return_amt, T1.sr_return_amt_inc_tax, T1.sr_fee, T1.sr_return_ship_cost, T1.sr_refunded_cash, T1.sr_reversed_charge, T1.sr_store_credit, T1.sr_net_loss FROM store_returns AS T1 JOIN customer AS T2 ON T1.sr_customer_sk = T2.c_customer_sk WHERE T2.c_birth_year = 2000 AND T2.c_birth_country = \"United States\" AND T2.c_birth_day = \"South Dakota\" AND T1.sr_return_quantity > T2.c_last_review_date_sk AND T1.sr_return_quantity > T2.c_first_shipto_date_sk AND T1.sr_return_quantity > T2.c_first_sales_date_sk AND T1.sr_return_quantity > T2.c_salutation AND T1.sr_return_quantity > T2.c_login AND T1.sr_return_quantity > T2.c_email_address AND T1.sr_return_quantity > T2.c_last_name AND T1.sr_return_quantity > T2.c_preferred_cust_flag AND T1.sr_return_quantity > T2.c_birth_month AND T1.sr_return_quantity > T2.c_birth_day AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_country AND T1.sr_return_quantity > T2.c_gmt_offset AND T1.sr_return_quantity > T2.c_tax_percentage AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_month AND T1.sr_return_quantity > T2.c_birth_day AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_country AND T1.sr_return_quantity > T2.c_street_number AND T1.sr_return_quantity > T2.c_street_name AND T1.sr_return_quantity > T2.c_street_type AND T1.sr_return_quantity > T2.c_suite_number AND T1.sr_return_quantity > T2.c_city AND T1.sr_return_quantity > T2.c_county AND T1.sr_return_quantity > T2.c_state AND T1.sr_return_quantity > T2.c_zip AND T1.sr_return_quantity > T2.c_country AND T1.sr_return_quantity > T2.c_gmt_offset AND T1.sr_return_quantity > T2.c_tax_percentage AND T1.sr_return_quantity > T2.c_preferred_cust_flag AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_month AND T1.sr_return_quantity > T2.c_birth_day AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_country AND T1.sr_return_quantity > T2.c_street_number AND T1.sr_return_quantity > T2.c_street_name AND T1.sr_return_quantity > T2.c_street_type AND T1.sr_return_quantity > T2.c_suite_number AND T1.sr_return_quantity > T2.c_city AND T1.sr_return_quantity > T2.c_county AND T1.sr_return_quantity > T2.c_state AND T1.sr_return_quantity > T2.c_zip AND T1.sr_return_quantity > T2.c_country AND T1.sr_return_quantity > T2.c_gmt_offset AND T1.sr_return_quantity > T2.c_tax_percentage AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_month AND T1.sr_return_quantity > T2.c_birth_day AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_country AND T1.sr_return_quantity > T2.c_gmt_offset AND T1.sr_return_quantity > T2.c_tax_percentage AND T1.sr_return_quantity > T2.c_preferred_cust_flag AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_month AND T1.sr_return_quantity > T2.c_birth_day AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_country AND T1.sr_return_quantity > T2.c_street_number AND T1.sr_return_quantity > T2.c_street_name AND T1.sr_return_quantity > T2.c_street_type AND T1.sr_return_quantity > T2.c_suite_number AND T1.sr_return_quantity > T2.c_city AND T1.sr_return_quantity > T2.c_county AND T1.sr_return_quantity > T2.c_state AND T1.sr_return_quantity > T2.c_zip AND T1.sr_return_quantity > T2.c_country AND T1.sr_return_quantity > T2.c_gmt_offset AND T1.sr_return_quantity > T2.c_tax_percentage AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_month AND T1.sr_return_quantity > T2.c_birth_day AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_country AND T1.sr_return_quantity > T2.c_gmt_offset AND T1.sr_return_quantity > T2.c_tax_percentage AND T1.sr_return_quantity > T2.c_preferred_cust_flag AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_month AND T1.sr_return_quantity > T2.c_birth_day AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_country AND T1.sr_return_quantity > T2.c_street_number AND T1.sr_return_quantity > T2.c_street_name AND T1.sr_return_quantity > T2.c_street_type AND T1.sr_return_quantity > T2.c_suite_number AND T1.sr_return_quantity > T2.c_city AND T1.sr_return_quantity > T2.c_county AND T1.sr_return_quantity > T2.c_state AND T1.sr_return_quantity > T2.c_zip AND T1.sr_return_quantity > T2.c_country AND T1.sr_return_quantity > T2.c_gmt_offset AND T1.sr_return_quantity > T2.c_tax_percentage AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth_month AND T1.sr_return_quantity > T2.c_birth_day AND T1.sr_return_quantity > T2.c_birth_year AND T1.sr_return_quantity > T2.c_birth\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#sample inferene running on final fine-tuned model\n",
    "test_prompt='''\n",
    "'\\n\\n[SYSTEM]:\"You are an expert Text-to-SQL generator assistant. Your goal is to provide correct SQL queries to the given text description. Your output only contains the SQL code. No explanation or introductory sentences surrounding the SQL response is needed. You are given schema information. Here is the schema information: \\n<tableName>store_returns</tableName>\\n<columns>sr_returned_date_sk,  sr_return_time_sk,  sr_item_sk,  sr_customer_sk,  sr_cdemo_sk,  sr_hdemo_sk,  sr_addr_sk,  sr_store_sk,  sr_reason_sk,  sr_ticket_number,  sr_return_quantity,  sr_return_amt,  sr_return_tax,  sr_return_amt_inc_tax,  sr_fee,  sr_return_ship_cost,  sr_refunded_cash,  sr_reversed_charge,  sr_store_credit,  sr_net_loss</columns>\\n<tableName>date_dim</tableName>\\n<columns>d_date_sk,  d_date_id,  d_date,  d_month_seq,  d_week_seq,  d_quarter_seq,  d_year,  d_dow,  d_moy,  d_dom,  d_qoy,  d_fy_year,  d_fy_quarter_seq,  d_fy_week_seq,  d_day_name,  d_quarter_name,  d_holiday,  d_weekend,  d_following_holiday,  d_first_dom,  d_last_dom,  d_same_day_ly,  d_same_day_lq,  d_current_day,  d_current_week,  d_current_month,  d_current_quarter,  d_current_year</columns>\\n<tableName>store</tableName>\\n<columns>s_store_sk,  s_store_id,  s_rec_start_date,  s_rec_end_date,  s_closed_date_sk,  s_store_name,  s_number_employees,  s_floor_space,  s_hours,  s_manager,  s_market_id,  s_geography_class,  s_market_desc,  s_market_manager,  s_division_id,  s_division_name,  s_company_id,  s_company_name,  s_street_number,  s_street_name,  s_street_type,  s_suite_number,  s_city,  s_county,  s_state,  s_zip,  s_country,  s_gmt_offset,  s_tax_percentage</columns>\\n<tableName>customer</tableName>\\n<columns>c_customer_sk,  c_customer_id,  c_current_cdemo_sk,  c_current_hdemo_sk,  c_current_addr_sk,  c_first_shipto_date_sk,  c_first_sales_date_sk,  c_salutation,  c_first_name,  c_last_name,  c_preferred_cust_flag,  c_birth_day,  c_birth_month,  c_birth_year,  c_birth_country,  c_login,  c_email_address,  c_last_review_date_sk</columns>\\n. Here are the 5 critical rules for the interactions you must abide: <rules> 1. Do not wrap the generated SQL code within SQL code markdown format. Also, do not include the SQL keyword in the beginning of the response. 2. If I don\\'t tell you to find the limited set of results, limit to 100. 3. Only use table and columns from the list provided 4. When performing aliasing, make sure to refer the aliased tables as alias.column_name and not as alias_column_name. 5. For US state names, use abbreviated forms. For example, for South Dakota state, use SD.</rules> \\n\\n\"Here is the user question:\"[/SYSTEM]\\n[HUMAN]: For the state of South Dakota in the year 2000, identify the first 100 customers, sorted by their IDs, whose returns are notably higher, exceeding the store\\'s average by more than 20%, indicating a trend of higher returns.\\n[/HUMAN]\\n\\n\n",
    "'''\n",
    "encoding = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config,\n",
    "      \n",
    "      \n",
    "    \n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "with open(f\"./test-q1-SD.txt\",\"w\") as file:\n",
    "  file.write(str(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
