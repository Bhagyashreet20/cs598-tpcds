{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear user, now you are entering an uncharted location that is not defined and you are very adventours. I warn you, you will be surprised by the superior performance of the model. However, you soon will realize the model only memorized the queries and did not learn. Given this inability, if you still want to pursue, the below code would work, but may not provide any good insights that you are seeking for yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINE-TUNING PRE-TRAINED LLAMA ON TPCDSA DATA\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import pipeline\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "train_splits=load_from_disk('./dataset//dataset-splits/train-split')\n",
    "val_splits=load_from_disk('./dataset/dataset-splits/val-split')\n",
    "test_splits=load_from_disk('./dataset/dataset-splits/test-split')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INFERENCE MODE -finetuned model\n",
    "\n",
    "#combine base and pretrained model - since pre-training using LORA is like adding new weights to the base model, that is why you need to merge the adapter and base model\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model_id = \"../models/7B/output\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "new_model_id = \"../models/llama-2-7b-finetuned-text2SQL\"\n",
    "new_model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto'\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, new_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True,device=\"cuda\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = base_model.generation_config\n",
    "generation_config.max_new_tokens = 2048\n",
    "#generation_config.temperature = 0.7\n",
    "#generation_config.top_p = 0.7\n",
    "#generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating responses for all test dataset\n",
    "import os\n",
    "import time\n",
    "query_time=\"\"\n",
    "for queryIdx in range(len(test_splits)):\n",
    "    start=time.time()\n",
    "    print(f\"started infernece for {queryIdx+1}\")\n",
    "    with torch.inference_mode():\n",
    "        encoding = tokenizer(test_splits[queryIdx][\"text\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(input_ids = encoding.input_ids, attention_mask = encoding.attention_mask,generation_config = generation_config) \n",
    "        query_time+= str(time.time() - start)  +\"\\n\"\n",
    "        print(f\"inference finished for query{queryIdx+1}\")\n",
    "        with open(f\"./test-results/llm-results/query_{queryIdx+1}.txt\",\"w\") as response_file:\n",
    "            response_file.write(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "            response_file.close()\n",
    "        print(\"completed writing response ot a file\")\n",
    "\n",
    "with open(f\"./test-results/querytime/query-time.txt\",\"w\") as time_file:\n",
    "            time_file.write(query_time)\n",
    "            time_file.close()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code randomly selects 99 queries from llama results (random sampling) to execute on db\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Paths for source and destination folders\n",
    "src_folder = './test-results/llm-responses'\n",
    "dest_folder = './test-results/sampled-queries'\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "# Generate a set of unique random numbers\n",
    "random_numbers = set(random.sample(range(1, 218), k=99))  # Adjust 'k' for the number of files you want to copy\n",
    "\n",
    "# Copy files based on generated numbers\n",
    "for number in random_numbers:\n",
    "    file_name = f'query_{number}.txt'\n",
    "    src_path = os.path.join(src_folder, file_name)\n",
    "    dest_path = os.path.join(dest_folder, file_name)\n",
    "\n",
    "    # Copy the file from the source to the destination\n",
    "    shutil.copy(src_path, dest_path)\n",
    "\n",
    "print(f\"Files successfully copied to {dest_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute randomly sampled 99 queries on duckdb\n",
    "\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the connection parameters\n",
    "query_path='./test-results/sampled-queries/'\n",
    "query_result='./test-results/db-results/llm-resp-db-results/results/'\n",
    "query_time_file='./test-results/db-results/llm-resp-db-results/querytime/log.txt'\n",
    "\n",
    "# Connect to the DB \n",
    "db_con = duckdb.connect()\n",
    "\n",
    "# Load the TPCDS database\n",
    "db_con.execute(\"IMPORT DATABASE '/workspace/data/cs598-tpcds/data/duckdb/tpcds_sf100'\")\n",
    "\n",
    "print(\"Loading of TPCDS DB complete.\")\n",
    "\n",
    "# Create a folder to store query results\n",
    "os.makedirs(query_result, exist_ok=True)\n",
    "\n",
    "query_files = [f for f in os.listdir(query_path) if os.path.isfile(os.path.join(query_path, f))]\n",
    "query_files = sorted(query_files)\n",
    "query_exec_time=\"\"\n",
    "# Iterate through the queries and execute them\n",
    "for query_file in query_files:\n",
    "    print(\"Executing \", query_file)\n",
    "    # extract query number from the query file name.\n",
    "    query_name = os.path.splitext(query_file)[0]\n",
    "\n",
    "    # Read the query from the file\n",
    "    with open(os.path.join(query_path, query_file), 'r') as f:\n",
    "        query=f.read()\n",
    "\n",
    "    # Execute the query and measure execution time\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = db_con.execute(query)\n",
    "        execution_time = time.time()-start_time\n",
    "    \n",
    "        # Save the query result to a file\n",
    "        output_file_path = query_result + query_name + \".csv\"\n",
    "        with open(output_file_path, \"w\") as out_file:\n",
    "            # print(\"Writing result:\", out_file)\n",
    "            for row in result.fetchall():\n",
    "                out_file.write(str(row) + \"\\n\")\n",
    "                # print(row)\n",
    "            out_file.close()    \n",
    "    except Exception as e: \n",
    "        print(f'Query {query_name} executed in error: {e}')\n",
    "    query_exec_time+= str(execution_time)+\"\\n\"\n",
    "        \n",
    "with open(query_time_file, \"a\") as time_file:\n",
    "    time_file.write(query_exec_time)\n",
    "\n",
    "    print(f'Query {query_name} executed in {execution_time:.2f} seconds')\n",
    "\n",
    "# Close the database connection\n",
    "db_con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute all sampled queries's golden answer on duckdb for comparison\n",
    "\n",
    "\n",
    "import duckdb\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Define the connection parameters\n",
    "query_path='./test-results/sampled-queries/'\n",
    "query_result='./test-results/db-results/golden-query-db-results/results/'\n",
    "query_time_file='./test-results/db-results/golden-query-db-results/querytime/log.txt'\n",
    "\n",
    "#load test splits as each of the sample has golden truth \n",
    "test_splits=load_from_disk('./dataset-splits/test-split')\n",
    "# Connect to the DB \n",
    "db_con = duckdb.connect()\n",
    "\n",
    "# Load the TPCDS database\n",
    "db_con.execute(\"IMPORT DATABASE '/workspace/data/cs598-tpcds/data/duckdb/tpcds_sf100'\")\n",
    "\n",
    "print(\"Loading of TPCDS DB complete.\")\n",
    "\n",
    "# Create a folder to store query results\n",
    "os.makedirs(query_result, exist_ok=True)\n",
    "\n",
    "query_files = [f for f in os.listdir(query_path) if os.path.isfile(os.path.join(query_path, f))]\n",
    "query_files = sorted(query_files)\n",
    "query_exec_time=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for query_file in query_files:\n",
    "    print(\"Executing \", query_file)\n",
    "    # extract query number from the query file name.\n",
    "    query_name = os.path.splitext(query_file)[0]\n",
    "    #regex to extract query number from query_10.txt \n",
    "    query_number=int(re.search(r'\\d+',query_name).group())\n",
    "    # print(query_name)\n",
    "    # print(\"query_number\",query_number-1)\n",
    "\n",
    "\n",
    "    sql_query=test_splits[query_number-1][\"output\"]\n",
    "    # print(\"sql_query\",sql_query)\n",
    "    # Execute the query and measure execution time\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = db_con.execute(sql_query)\n",
    "        execution_time = time.time()-start_time\n",
    "    \n",
    "        # Save the query result to a file\n",
    "        output_file_path = query_result + query_name + \".csv\"\n",
    "        with open(output_file_path, \"w\") as out_file:\n",
    "            # print(\"Writing result:\", out_file)\n",
    "            for row in result.fetchall():\n",
    "                out_file.write(str(row) + \"\\n\")\n",
    "                # print(row)\n",
    "            out_file.close()    \n",
    "    except Exception as e: \n",
    "        print(f'Query {query_name} executed in error: {e}')\n",
    "    query_exec_time+= str(execution_time)+\"\\n\"\n",
    "        \n",
    "with open(query_time_file, \"a\") as time_file:\n",
    "    time_file.write(query_exec_time)\n",
    "\n",
    "    print(f'Query {query_name} executed in {execution_time:.2f} seconds')\n",
    "# Close the database connection\n",
    "db_con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample inferene running on final fine-tuned model\n",
    "test_prompt='''\n",
    "'\\n\\n[SYSTEM]:\"You are an expert Text-to-SQL generator assistant. Your goal is to provide correct SQL queries to the given text description. Your output only contains the SQL code. No explanation or introductory sentences surrounding the SQL response is needed. You are given schema information. Here is the schema information: \\n<tableName>store_returns</tableName>\\n<columns>sr_returned_date_sk,  sr_return_time_sk,  sr_item_sk,  sr_customer_sk,  sr_cdemo_sk,  sr_hdemo_sk,  sr_addr_sk,  sr_store_sk,  sr_reason_sk,  sr_ticket_number,  sr_return_quantity,  sr_return_amt,  sr_return_tax,  sr_return_amt_inc_tax,  sr_fee,  sr_return_ship_cost,  sr_refunded_cash,  sr_reversed_charge,  sr_store_credit,  sr_net_loss</columns>\\n<tableName>date_dim</tableName>\\n<columns>d_date_sk,  d_date_id,  d_date,  d_month_seq,  d_week_seq,  d_quarter_seq,  d_year,  d_dow,  d_moy,  d_dom,  d_qoy,  d_fy_year,  d_fy_quarter_seq,  d_fy_week_seq,  d_day_name,  d_quarter_name,  d_holiday,  d_weekend,  d_following_holiday,  d_first_dom,  d_last_dom,  d_same_day_ly,  d_same_day_lq,  d_current_day,  d_current_week,  d_current_month,  d_current_quarter,  d_current_year</columns>\\n<tableName>store</tableName>\\n<columns>s_store_sk,  s_store_id,  s_rec_start_date,  s_rec_end_date,  s_closed_date_sk,  s_store_name,  s_number_employees,  s_floor_space,  s_hours,  s_manager,  s_market_id,  s_geography_class,  s_market_desc,  s_market_manager,  s_division_id,  s_division_name,  s_company_id,  s_company_name,  s_street_number,  s_street_name,  s_street_type,  s_suite_number,  s_city,  s_county,  s_state,  s_zip,  s_country,  s_gmt_offset,  s_tax_percentage</columns>\\n<tableName>customer</tableName>\\n<columns>c_customer_sk,  c_customer_id,  c_current_cdemo_sk,  c_current_hdemo_sk,  c_current_addr_sk,  c_first_shipto_date_sk,  c_first_sales_date_sk,  c_salutation,  c_first_name,  c_last_name,  c_preferred_cust_flag,  c_birth_day,  c_birth_month,  c_birth_year,  c_birth_country,  c_login,  c_email_address,  c_last_review_date_sk</columns>\\n. Here are the 5 critical rules for the interactions you must abide: <rules> 1. Do not wrap the generated SQL code within SQL code markdown format. Also, do not include the SQL keyword in the beginning of the response. 2. If I don\\'t tell you to find the limited set of results, limit to 100. 3. Only use table and columns from the list provided 4. When performing aliasing, make sure to refer the aliased tables as alias.column_name and not as alias_column_name. 5. For US state names, use abbreviated forms. For example, for South Dakota state, use SD.</rules> \\n\\n\"Here is the user question:\"[/SYSTEM]\\n[HUMAN]: For the state of South Dakota in the year 2000, identify the first 100 customers, sorted by their IDs, whose returns are notably higher, exceeding the store\\'s average by more than 20%, indicating a trend of higher returns.\\n[/HUMAN]\\n\\n\n",
    "'''\n",
    "encoding = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config,\n",
    "      \n",
    "      \n",
    "    \n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "with open(f\"./test-q1-SD.txt\",\"w\") as file:\n",
    "  file.write(str(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
