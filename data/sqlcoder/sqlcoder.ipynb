{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig\n",
    "#FINE-TUNING PRE-TRAINED LLAMA ON TPCDSA DATA\n",
    "\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf50266c3664fb89d390eccef623312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"codellama/CodeLlama-34b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto',\n",
    "    use_cache=True\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema-induced prompt templates are provided in the `prompt_folder`. The below script automatically picks the prompts and generates the requires schema specific to each query and then executes on the SQLCoder-34B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve the customer IDs of customers who have returned items significantly more frequently (over 20% above the average return rate) than the average customer return rate for a store in the state of South Dakota in the year 2000. This query calculates the total returns for each customer at each store and compares it to the store's average returns for the specified year. It then retrieves the customer IDs of those whose returns exceed the calculated threshold. The results are sorted by customer ID, and a limit of 100 records is applied\n",
      "\n",
      "CREATE TABLE store_returns (  sr_returned_date_sk,  sr_return_time_sk,  sr_item_sk,  sr_customer_sk,  sr_cdemo_sk,  sr_hdemo_sk,  sr_addr_sk,  sr_store_sk,  sr_reason_sk,  sr_ticket_number,  sr_return_quantity,  sr_return_amt,  sr_return_tax,  sr_return_amt_inc_tax,  sr_fee,  sr_return_ship_cost,  sr_refunded_cash,  sr_reversed_charge,  sr_store_credit,  sr_net_loss );\n",
      "\n",
      "CREATE TABLE date_dim (  d_date_sk,  d_date_id,  d_date,  d_month_seq,  d_week_seq,  d_quarter_seq,  d_year,  d_dow,  d_moy,  d_dom,  d_qoy,  d_fy_year,  d_fy_quarter_seq,  d_fy_week_seq,  d_day_name,  d_quarter_name,  d_holiday,  d_weekend,  d_following_holiday,  d_first_dom,  d_last_dom,  d_same_day_ly,  d_same_day_lq,  d_current_day,  d_current_week,  d_current_month,  d_current_quarter,  d_current_year );\n",
      "\n",
      "CREATE TABLE store (  s_store_sk,  s_store_id,  s_rec_start_date,  s_rec_end_date,  s_closed_date_sk,  s_store_name,  s_number_employees,  s_floor_space,  s_hours,  s_manager,  s_market_id,  s_geography_class,  s_market_desc,  s_market_manager,  s_division_id,  s_division_name,  s_company_id,  s_company_name,  s_street_number,  s_street_name,  s_street_type,  s_suite_number,  s_city,  s_county,  s_state,  s_zip,  s_country,  s_gmt_offset,  s_tax_percentage );\n",
      "\n",
      "CREATE TABLE customer (  c_customer_sk,  c_customer_id,  c_current_cdemo_sk,  c_current_hdemo_sk,  c_current_addr_sk,  c_first_shipto_date_sk,  c_first_sales_date_sk,  c_salutation,  c_first_name,  c_last_name,  c_preferred_cust_flag,  c_birth_day,  c_birth_month,  c_birth_year,  c_birth_country,  c_login,  c_email_address,  c_last_review_date_sk );\n",
      "../../data/sqlcoder/question_and_schema/combine1.txt\n",
      "started infernece for 1\n",
      "inference finished for query1\n",
      "completed writing response to a file\n"
     ]
    }
   ],
   "source": [
    "#generating responses for all test dataset\n",
    "import os\n",
    "import time\n",
    "import sqlparse\n",
    "query_time=\"\"\n",
    "prompt_folder=\"../../data/sqlcoder/question_and_schema\"\n",
    "\n",
    "for queryIdx in range(1):\n",
    "    start=time.time()\n",
    "    with open(f\"{prompt_folder}/combine{queryIdx+1}.txt\",\"r\") as file:\n",
    "        codellama_prompt= file.read()\n",
    "        print(codellama_prompt)\n",
    "    print(f\"started infernece for {queryIdx+1}\")\n",
    "    inputs = tokenizer(codellama_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = base_model.generate(**inputs,num_return_sequences=1,eos_token_id=eos_token_id,pad_token_id=eos_token_id,max_new_tokens=1500,do_sample=False,num_beams=1)\n",
    "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    query_time+= str(time.time() - start)  +\"\\n\"\n",
    "    print(f\"inference finished for query{queryIdx+1}\")\n",
    "    with open(f\"../../data/sqlcoder/queries/query_{queryIdx+1}.txt\",\"w\") as response_file:\n",
    "        content=sqlparse.format(outputs[0].split(\"```sql\")[-1], reindent=True)\n",
    "        response_file.write(content)\n",
    "        response_file.close()\n",
    "        print(\"completed writing response to a file\")\n",
    "\n",
    "with open(f\"../../data/sqlcoder/querytime/query-time.txt\",\"w\") as time_file:\n",
    "            time_file.write(query_time)\n",
    "            time_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "\n",
    "As discussed in the article, automatic execution of the SQL queries directly with duckDB cannot work in case of SQLCoder-34B because of the above boilerplate text. One requires to manually edit the file stored in the `data/sqlcoder/queries/query_{queryIdx+1}.txt` (Yep, we know it is a headache but it is what baseline model generates as of now). This requires constant human intervention. \n",
    "\n",
    "To save time and resources, we have preloaded the sqlcoder queries folder with only the successfully executeable queires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of TPCDS DB complete.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the connection parameters\n",
    "query_path='../../data/sqlcoder/queries/'\n",
    "query_result='../../data/sqlcoder/results/'\n",
    "query_time_file='../../data/sqlcoder/results/querytime/log.txt'\n",
    "\n",
    "# Connect to the DB \n",
    "db_con = duckdb.connect()\n",
    "\n",
    "# Load the TPCDS database\n",
    "db_con.execute(\"IMPORT DATABASE '/workspace/data/cs598-tpcds/data/duckdb/tpcds_sf100'\")\n",
    "\n",
    "print(\"Loading of TPCDS DB complete.\")\n",
    "\n",
    "# Create a folder to store query results\n",
    "os.makedirs(query_result, exist_ok=True)\n",
    "\n",
    "query_files = [f for f in os.listdir(query_path) if os.path.isfile(os.path.join(query_path, f))]\n",
    "query_files = sorted(query_files)\n",
    "query_exec_time=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing  query_1.txt\n",
      "Query query_1 executed in error: Parser Error: syntax error at or near \"Retrieve\"\n",
      "LINE 1: Retrieve the customer IDs of customers ...\n",
      "        ^\n",
      "Executing  query_22.sql\n",
      "Query query_22 executed in 0.35 seconds\n",
      "Executing  query_37.sql\n",
      "Query query_37 executed in 0.01 seconds\n",
      "Executing  query_95.sql\n",
      "Query query_95 executed in 0.01 seconds\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Iterate through the queries and execute them\n",
    "for query_file in query_files:\n",
    "    print(\"Executing \", query_file)\n",
    "    # extract query number from the query file name.\n",
    "    query_name = os.path.splitext(query_file)[0]\n",
    "\n",
    "    # Read the query from the file\n",
    "    with open(os.path.join(query_path, query_file), 'r') as f:\n",
    "        query=f.read()\n",
    "\n",
    "    # Execute the query and measure execution time\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = db_con.execute(query)\n",
    "        execution_time = time.time()-start_time\n",
    "    \n",
    "        # Save the query result to a file\n",
    "        output_file_path = query_result + query_name + \".csv\"\n",
    "        with open(output_file_path, \"w\") as out_file:\n",
    "            for row in result.fetchall():\n",
    "                out_file.write(str(row) + \"\\n\")\n",
    "        with open(query_time_file, \"a\") as time_file:\n",
    "                time_file.write(f'Query {query_name} executed in {execution_time:.2f} seconds' + \"\\n\")\n",
    "\n",
    "        print(f'Query {query_name} executed in {execution_time:.2f} seconds')\n",
    "    except Exception as e: \n",
    "        print(f'Query {query_name} executed in error: {e}')\n",
    "# Close the database connection\n",
    "db_con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
