{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creates spider prompts which serves as input in the finetune dataset of spider.\n",
    "#{input: prompt, output:golden query}\n",
    "import os \n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context': 'CREATE TABLE head (name VARCHAR, born_state VARCHAR, age VARCHAR)',\n",
       " 'answer': 'SELECT name, born_state, age FROM head ORDER BY age',\n",
       " 'question': 'List the name, born state and age of the heads of departments ordered by age.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fusion of spider and wiki-sql\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# data=[]\n",
    "# for queryIdx in range(len(dataset)):\n",
    "#     # data.append({\n",
    "#     #     \"input\":generate_prompt(query),\n",
    "#     #     \"output\":query[\"answer\"]\n",
    "#     # })\n",
    "#     with open(f'/workspace/data/finetuning/spider/input/query_{queryIdx+1}.txt',\"w\") as input_file:\n",
    "#         input_file.write(generate_prompt(dataset[queryIdx]))\n",
    "#     with open(f'/workspace/data/finetuning/spider/output/query_{queryIdx+1}.txt',\"w\") as output_file:\n",
    "#         output_file.write(dataset[queryIdx][\"answer\"])\n",
    "#     input_file.close()\n",
    "#     output_file.close()\n",
    "\n",
    "\n",
    "#json_string = json.dumps(data, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=[]\n",
    "prompt=f'''\n",
    "[SYSTEM]:\"You are an expert Text-to-SQL generator assistant. Your goal is to provide correct SQL queries to the given text description. Your output only contains the SQL code. No explanation or introductory sentences surrounding the SQL response is needed. You are given schema information. Here is the schema information: \n",
    "{\"{schema_info}\"}. Here are the 5 critical rules for the interactions you must abide: <rules> 1. Do not wrap the generated SQL code within SQL code markdown format. Also, do not include the SQL keyword in the beginning of the response. 2. If I don't tell you to find the limited set of results, limit to 100. 3. Only use table and columns from the list provided 4. When performing aliasing, make sure to refer the aliased tables as alias.column_name and not as alias_column_name. 5. For US state names, use abbreviated forms. For example, for South Dakota state, use SD.</rules> \n",
    "\n",
    "\"Here is the user question:\"[/SYSTEM]\n",
    "[HUMAN]:{\"{user_input}\"}[/HUMAN]\n",
    "'''\n",
    "\n",
    "for queryIdx in range(len(dataset)):\n",
    "    user_input=prompt.format(schema_info=dataset[queryIdx][\"context\"],user_input=dataset[queryIdx][\"question\"])\n",
    "    output=dataset[queryIdx][\"answer\"]\n",
    "    data.append({\n",
    "        \"input\":user_input,\n",
    "        \"output\":output,\n",
    "        \"text\": f'''\\n{user_input}\\n[SEP] {output}'''\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "json_string = json.dumps(data, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open('/workspace/data/finetuning/spider/spider_wikisql_dataset.json', 'w') as file:\n",
    "    file.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create modified dataset including prompt for fine-tuning \n",
    "import json\n",
    "file_path = \"/workspace/data/finetuning/dataset/sqlize-dataset.json\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    sqlize_data = json.load(file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n"
     ]
    }
   ],
   "source": [
    "print(len(sqlize_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_prompt=f'''\n",
    "[SYSTEM]:\"You are an expert Text-to-SQL generator assistant. Your goal is to provide correct SQL queries to the given text description. Your output only contains the SQL code. No explanation or introductory sentences surrounding the SQL response is needed. You are given schema information. Here is the schema information: \n",
    "{\"{schema_info}\"}. Here are the 5 critical rules for the interactions you must abide: <rules> 1. Do not wrap the generated SQL code within SQL code markdown format. Also, do not include the SQL keyword in the beginning of the response. 2. If I don't tell you to find the limited set of results, limit to 100. 3. Only use table and columns from the list provided 4. When performing aliasing, make sure to refer the aliased tables as alias.column_name and not as alias_column_name. 5. For US state names, use abbreviated forms. For example, for South Dakota state, use SD.</rules> \n",
    "\n",
    "\"Here is the user question:\"[/SYSTEM]\n",
    "[HUMAN]:{\"{user_input}\"}[/HUMAN]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_data=[]\n",
    "for queryIdx in range(len(sqlize_data)):\n",
    "    user_input=finetuned_prompt.format(schema_info=sqlize_data[queryIdx][\"schema\"],user_input=sqlize_data[queryIdx][\"nlq\"])\n",
    "    output=sqlize_data[queryIdx][\"sql\"]\n",
    "    finetuned_data.append({\n",
    "        \"id\":sqlize_data[queryIdx][\"id\"],\n",
    "        \"input\":user_input,\n",
    "        \"output\":output,\n",
    "        \"text\": f'''\\n{user_input}\\n[SEP] {output}'''\n",
    "    })\n",
    "\n",
    "json_string = json.dumps(finetuned_data, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open('/workspace/data/finetuning/dataset/sqlize-finetuned-dataset.json', 'w') as file:\n",
    "    file.write(json_string)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
