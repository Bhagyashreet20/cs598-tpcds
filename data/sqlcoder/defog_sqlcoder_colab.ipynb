{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnhvqa6ZRscb"
      },
      "source": [
        "#SQLCoder2\n",
        "Run the cells below to run inference on our text-to-SQL LLM: SQLCoder2. This loads the model with int4 quantization. You can use a larger GPU to load the int8 and float16 variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKYJQvVmSEWs"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bAMjQKJfG3d"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers bitsandbytes accelerate sqlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzllQomZfQnj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hztT0MXkfRs1"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt4ilTkpTMoL"
      },
      "source": [
        "##Download the Model\n",
        "Use an A100 on Colab Pro (or any system with >30GB VRAM on your own machine) to load this in bf16. If unavailable, use a GPU with minimum 20GB VRAM to load this in 8bit, or with minimum 12GB of VRAM to load in 4bit. On Colab, it works with a V100 but crashes on a T4.\n",
        "\n",
        "Downloading the model and then loading it to memory step takes around 10 minutes the first time. So please be patient :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mNT25UOfSMw"
      },
      "outputs": [],
      "source": [
        "model_name = \"codellama/CodeLlama-34b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    # load_in_8bit=True,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    use_cache=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bq7KgZaZiBf_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_DYvULexlSd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sqlparse\n",
        "\n",
        "# Function to extract question and schema from file content\n",
        "def extract_question_schema(content):\n",
        "    split_index = content.index(\"CREATE\")\n",
        "    question = content[:split_index].strip()\n",
        "    schema = content[split_index:].strip()\n",
        "    return question, schema\n",
        "\n",
        "# Function to generate SQL query\n",
        "def generate_sql(question, schema):\n",
        "        prompt = \"\"\"### Task\n",
        "    Generate a SQL query to answer the following question:\n",
        "    `{question}`\n",
        "\n",
        "    ### Database Schema\n",
        "    This query will run on a database whose schema is represented in this string:\n",
        "    `{schema}`\n",
        "    ### SQL\n",
        "    Given the database schema, here is the SQL query that answers `{question}`:\n",
        "    ```sql\n",
        "    \"\"\".format(question=question)\n",
        "\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=eos_token_id,\n",
        "        pad_token_id=eos_token_id,\n",
        "        max_new_tokens=400,\n",
        "        do_sample=False,\n",
        "        num_beams=1,\n",
        "\n",
        "    )\n",
        "\n",
        "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    # empty cache so that you do generate more results w/o memory crashing\n",
        "    # particularly important on Colab â€“ memory management is much more straightforward\n",
        "    # when running on an inference service\n",
        "\n",
        "\n",
        "    print(sqlparse.format(outputs[0].split(\"```sql\")[-1], reindent=True))\n",
        "    return generated_sql\n",
        "\n",
        "input_folder_path = 'data/sqlcoder/question_and_schema/'\n",
        "output_folder_path = 'data/sqlcoder/queries/'\n",
        "\n",
        "for file_name in os.listdir(input_folder_path):\n",
        "    if file_name.endswith('.txt'):\n",
        "        with open(os.path.join(input_folder_path, file_name), 'r') as file:\n",
        "            content = file.read()\n",
        "\n",
        "            # Extract question and schema\n",
        "            question, schema = extract_question_schema(content)\n",
        "\n",
        "            # Generate SQL query\n",
        "            generated_sql = generate_sql(question, schema)\n",
        "\n",
        "            # Output the results\n",
        "            output_file_name = f\"{file_name}_output.sql\"\n",
        "            with open(os.path.join(output_folder_path, output_file_name), 'w') as output_file:\n",
        "                output_file.write(generated_sql)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
